[{"id":0,"href":"/photography-basics/docs/wstep-budowa-aparatu/","title":"Jak jest zbudowany aparat","section":"Docs","content":" Wprowadzenie do budowy aparatu fotograficznego # Zrozumienie działania aparatu fotograficznego rozpoczyna się od poznania jego fundamentalnej budowy i zasady, według której światło jest przekształcane w obraz. Choć współczesne aparaty, zarówno cyfrowe, jak i wciąż cenione przez niektórych analogowe, są urządzeniami niezwykle złożonymi, ich korzenie sięgają prostej koncepcji znanej od wieków.\nPodstawowa zasada działania każdego aparatu fotograficznego, niezależnie od jego zaawansowania technologicznego, wywodzi się bezpośrednio od camera obscura (łac. ciemna komnata).\nKluczowe moduły aparatu fotograficznego i ich podstawowe funkcje # Każdy aparat fotograficzny, czy to klasyczny model na kliszę, czy nowoczesna cyfrówka, składa się z kilku kluczowych modułów, które współpracują ze sobą, aby stworzyć zdjęcie:\nKorpus (Body): Stanowi szkielet aparatu, chroniąc jego delikatne wewnętrzne komponenty przed uszkodzeniami mechanicznymi i wpływem czynników zewnętrznych. Zapewnia również ergonomiczny chwyt i rozmieszczenie elementów sterujących. W korpusie mieści się elektronika sterująca, system zasilania, a w aparatach cyfrowych także procesor obrazu i gniazdo karty pamięci. Obiektyw (Lens): Często nazywany \u0026ldquo;okiem\u0026rdquo; aparatu, jest złożonym układem optycznym składającym się z wielu soczewek. Jego głównym zadaniem jest zebranie promieni świetlnych odbitych od fotografowanej sceny i skupienie ich w taki sposób, aby utworzyły ostry, rzeczywisty i najczęściej pomniejszony obraz na powierzchni materiału światłoczułego. Jakość obiektywu ma krytyczny wpływ na finalną jakość zdjęcia, w tym na jego ostrość, kontrast, odwzorowanie kolorów i obecność lub brak wad optycznych. Przysłona (Aperture): Jest to mechanizm znajdujący się zazwyczaj wewnątrz obiektywu, składający się z ruchomych blaszek (listków), które tworzą otwór o zmiennej średnicy. Główną funkcją przysłony jest regulacja ilości światła wpadającego przez obiektyw na materiał światłoczuły. Wielkość otworu przysłony wpływa również bezpośrednio na głębię ostrości zdjęcia. Migawka (Shutter): To precyzyjny mechanizm kontrolujący czas, przez jaki światło ma dostęp do materiału światłoczułego – jest to tzw. czas naświetlania lub czas ekspozycji. W stanie spoczynku migawka zasłania materiał światłoczuły. Jej otwarcie i zamknięcie inicjuje proces rejestracji obrazu. Materiał światłoczuły (Sensor/Film): Element, na którym rejestrowany jest obraz. W fotografii analogowej jest to błona fotograficzna (klisza), pokryta emulsją zawierającą związki srebra reagujące na światło w procesie chemicznym. W fotografii cyfrowej jest to matryca światłoczuła (np. CCD lub CMOS), składająca się z milionów pikseli, które przekształcają padające na nie fotony w sygnał elektryczny. Wizjer (Viewfinder): Umożliwia fotografowi podgląd kadru, czyli tego, co zostanie zarejestrowane na zdjęciu, jeszcze przed jego wykonaniem.2 Wizjery mogą być optyczne (jak w lustrzankach, gdzie obraz z obiektywu jest kierowany do oka za pomocą lustra i pryzmatu) lub elektroniczne (jak w aparatach bezlusterkowych i kompaktowych, gdzie na małym ekranie wyświetlany jest obraz bezpośrednio z matrycy). System ustawiania ostrości (Focusing System): Mechanizm pozwalający na precyzyjne ustawienie soczewek obiektywu tak, aby wybrany fragment sceny był odwzorowany na materiale światłoczułym jako ostry. Może być manualny (ręczny, sterowany przez fotografa) lub automatyczny (autofokus, AF), gdzie aparat sam ustawia ostrość na podstawie analizy obrazu. Wzajemne oddziaływanie modułów w procesie tworzenia obrazu # Proces tworzenia zdjęcia to harmonijna współpraca wszystkich wymienionych komponentów. Światło odbite od fotografowanej sceny wpada do aparatu przez obiektyw. Zespół soczewek w obiektywie skupia te promienie. Wielkość otworu przysłony, znajdującej się w obiektywie, decyduje o tym, jak duża część tego światła (jego natężenie) dotrze dalej. Następnie, po naciśnięciu spustu, migawka otwiera się na precyzyjnie określony czas, umożliwiając światłu dotarcie do materiału światłoczułego. W tym momencie na kliszy lub matrycy powstaje utajony obraz, który w przypadku kliszy wymaga dalszej obróbki chemicznej, a w przypadku matrycy jest przetwarzany elektronicznie na obraz cyfrowy. Fotograf przez cały czas kontroluje kompozycję kadru za pomocą wizjera i dba o to, by główny motyw był ostry, korzystając z systemu ustawiania ostrości. Każdy z tych elementów odgrywa kluczową rolę, a ich wzajemne ustawienia (czas naświetlania, wartość przysłony, czułość materiału światłoczułego, punkt ostrości) decydują o finalnym wyglądzie fotografii.\nZwięzłe porównanie budowy aparatu analogowego i cyfrowego # Choć fundamentalne zasady optyczne pozostają te same, istnieją kluczowe różnice w budowie aparatów analogowych i cyfrowych, wynikające przede wszystkim z odmiennego sposobu rejestracji i przetwarzania obrazu.\nGłówną i najbardziej oczywistą różnicą jest rodzaj materiału światłoczułego. W aparatach analogowych jest to błona fotograficzna (klisza), na której obraz zapisywany jest w wyniku reakcji chemicznych zachodzących w emulsji światłoczułej pod wpływem światła. W aparatach cyfrowych rolę tę pełni matryca światłoczuła (sensor), która przekształca energię świetlną fotonów na sygnały elektryczne, a następnie na dane cyfrowe.\nTa podstawowa różnica pociąga za sobą kolejne. Zamiast komory na kliszę i mechanizmu jej przewijania, aparaty cyfrowe posiadają gniazdo na kartę pamięci, gdzie zapisywane są pliki zdjęciowe. Proces \u0026ldquo;wywoływania\u0026rdquo; w fotografii cyfrowej odbywa się natychmiastowo (lub później, w komputerze) i polega na elektronicznym przetwarzaniu danych z matrycy, a nie na kąpielach chemicznych.\nWspółczesne aparaty cyfrowe są wyposażone w zaawansowaną elektronikę i oprogramowanie wewnętrzne (firmware), które zarządzają nie tylko procesem rejestracji obrazu, ale także wieloma dodatkowymi funkcjami, takimi jak systemy autofokusa, pomiaru światła, balansu bieli, stabilizacji obrazu, nagrywania wideo, a nawet komunikacji bezprzewodowej. Choć te funkcje znacznie ułatwiają uzyskanie technicznie poprawnych zdjęć, a nawet rozszerzają kreatywne możliwości, mogą jednocześnie oddalać użytkownika od zrozumienia podstawowych mechanizmów działania aparatu, jeśli polega on wyłącznie na trybach automatycznych. Zrozumienie, jak przysłona, czas naświetlania i czułość ISO wpływają na obraz, oraz dlaczego aparat w trybie automatycznym podejmuje określone decyzje, jest kluczowe dla świadomego fotografowania i pełnego wykorzystania potencjału nawet najbardziej zaawansowanego sprzętu.\n"},{"id":1,"href":"/photography-basics/docs/camera-obscura/","title":"Jak Działa Camera Obscura","section":"Docs","content":"Camera Obscura, z łaciny \u0026ldquo;ciemny pokój\u0026rdquo;, to proste urządzenie optyczne, które stanowiło pierwowzór aparatu fotograficznego. Jej działanie opiera się na fundamentalnej właściwości światła i pozwala na rzutowanie rzeczywistego, choć odwróconego, obrazu otoczenia na powierzchnię wewnątrz zaciemnionej przestrzeni. Zrozumienie jej mechanizmu to podróż do samych podstaw optyki.\nZasada Działania: Magia Prostych Linii Światła # Wbrew pozorom, w Camera Obscura nie ma żadnej \u0026ldquo;magii\u0026rdquo; – jest czysta fizyka. Kluczem do zrozumienia jej działania jest uświadomienie sobie, że światło rozchodzi się w liniach prostych.\nWyobraźmy sobie całkowicie ciemne pomieszczenie lub pudełko. W jednej z jego ścianek robimy niewielki otwór. Każdy punkt na zewnątrz tego pomieszczenia odbija światło we wszystkich kierunkach. Jednak do wnętrza naszego \u0026ldquo;ciemnego pokoju\u0026rdquo; mogą wpaść tylko te promienie, które trafią prosto w nasz mały otwór.\nI tu dzieje się cała \u0026ldquo;magia\u0026rdquo;:\nPromienie światła odbite od górnej części obserwowanego obiektu (np. czubka drzewa) wpadają przez otwór i – biegnąc dalej prosto – padają na dolną część przeciwległej ściany wewnątrz. Analogicznie, promienie z dolnej części obiektu (np. trawy pod drzewem) wpadają przez otwór i lądują na górnej części tej samej ściany. To samo dotyczy stron: promienie z lewej strony sceny trafiają na prawą stronę ekranu, a te z prawej – na lewą. W ten sposób na wewnętrznej ścianie powstaje obraz, który jest:\nOdwrócony góra-dół. Odwrócony prawo-lewo. Rzeczywisty, co oznacza, że można go fizycznie zobaczyć na ekranie (w przeciwieństwie do obrazu w lustrze, który jest pozorny). Rola Otworu: Kompromis Między Ostrością a Jasnością # Wielkość otworu (apertury) ma kluczowe znaczenie dla jakości uzyskanego obrazu i jest sednem kompromisu w działaniu Camera Obscura.\nMały otwór: Jeśli otwór jest bardzo mały (niemal punktowy), przepuszcza bardzo wąskie wiązki światła z każdego punktu obserwowanej sceny. Na ekranie każdy taki punkt jest odwzorowany jako bardzo mała plamka. Dzięki temu obraz jest bardzo ostry i szczegółowy. Ceną za to jest jednak niska jasność – przez mały otwór wpada niewiele światła, więc obraz jest ciemny.\nDuży otwór: Zwiększenie średnicy otworu wpuszcza do środka znacznie więcej światła, co sprawia, że obraz staje się jasny i wyraźny. Niestety, dzieje się to kosztem ostrości. Duży otwór można sobie wyobrazić jako zbiór wielu małych otworków położonych tuż obok siebie. Każdy z nich tworzy własną, lekko przesuniętą wersję obrazu. Te nakładające się na siebie obrazy powodują, że każdy punkt sceny jest reprezentowany na ekranie nie jako punkt, ale jako większe kółko (tzw. krążek rozmycia). W efekcie cały obraz staje się rozmyty i nieostry.\nZatem idealny otwór to taki, który jest wystarczająco mały, by zapewnić dobrą ostrość, a jednocześnie na tyle duży, by obraz był wystarczająco jasny do obserwacji.\nEwolucja i Zastosowania: Od Astronomii po Malarstwo # Zjawisko Camera Obscura było znane już w starożytności – opisywali je m.in. chiński filozof Mo Di oraz Arystoteles, który obserwował w ten sposób zaćmienia Słońca. Przez wieki urządzenie to było udoskonalane.\nPrzełomem było dodanie soczewki w otworze, co przypisuje się m.in. Leonardowi da Vinci. Soczewka pozwalała na użycie znacznie większego otworu, a co za tym idzie – uzyskanie jasnego obrazu, jednocześnie skupiając promienie światła, co zapewniało jego ostrość. Później dodano również lustra, które odwracały obraz z powrotem do prawidłowej orientacji, co było niezwykle pomocne dla artystów.\nCamera Obscura była nieocenionym narzędziem dla:\nAstronomów: Do bezpiecznej obserwacji plam na Słońcu i zaćmień. Malarzy: Wielcy mistrzowie, tacy jak Johannes Vermeer czy Canaletto, prawdopodobnie używali jej do uzyskania idealnej perspektywy i realistycznego oddania detali w swoich dziełach. Obraz rzutowany na płótno można było po prostu odrysować. Naukowców i filozofów: Służyła jako model do zrozumienia działania ludzkiego oka. Ostatecznie, gdy w XIX wieku odkryto materiały światłoczułe, które mogły trwale zapisać obraz powstający wewnątrz \u0026ldquo;ciemnego pokoju\u0026rdquo;, Camera Obscura przekształciła się w aparat fotograficzny, rewolucjonizując sposób, w jaki postrzegamy i dokumentujemy świat.\n1. Czym jest Głębia Ostrości w Zwykłym Aparacie? # W aparacie z obiektywem (soczewką), głębia ostrości to zakres odległości (od bliży do dali), w którym obiekty na zdjęciu wydają się akceptowalnie ostre.\nOgniskowanie: Soczewka działa poprzez zginanie promieni światła tak, aby zbiegały się one w jednym punkcie na płaszczyźnie materiału światłoczułego (np. matrycy cyfrowej). Aby obiekt był idealnie ostry, musisz \u0026ldquo;ustawić ostrość\u0026rdquo;, czyli fizycznie przesunąć soczewkę tak, aby punkt zbiegu promieni z tego obiektu idealnie pokrywał się z matrycą. Płaszczyzna Ostrości: W danym momencie tylko jedna, konkretna odległość od aparatu jest idealnie \u0026ldquo;w ognisku\u0026rdquo;. To jest tzw. płaszczyzna ostrości. Rozmycie: Obiekty znajdujące się przed lub za tą płaszczyzną stają się stopniowo coraz bardziej rozmyte. Dzieje się tak, ponieważ promienie światła z tych nieostrych obiektów nie zbiegają się w punkt na matrycy, ale tworzą tzw. krążek rozmycia (ang. circle of confusion). Jeśli ten krążek jest wystarczająco mały, nasze oko nadal postrzega go jako punkt i obiekt wydaje się ostry. Głębia ostrości to właśnie ten zakres, gdzie krążki rozmycia są akceptowalnie małe. 2. Dlaczego w Camera Obscura Jest Inaczej? Brak Ogniskowania. # Camera Obscura nie ma soczewki, która by zginała i ogniskowała światło. Jej jedynym elementem optycznym jest maleńki otwór (pinhole).\nTutaj leży klucz do całej zagadki: Otwór nie skupia światła. On je jedynie selekcjonuje.\nZ każdego punktu w obserwowanej scenie – czy jest on 1 metr od Camera Obscura, czy 10 kilometrów dalej – wychodzą promienie światła we wszystkich kierunkach. Maleńki otwór działa jak bramka, która przepuszcza tylko bardzo, bardzo wąską wiązkę promieni z każdego z tych punktów.\nPonieważ wiązka jest tak wąska, \u0026ldquo;krążek rozmycia\u0026rdquo;, który tworzy na ekranie, jest z definicji niezwykle mały – jego rozmiar jest w przybliżeniu równy rozmiarowi samego otworu. Co najważniejsze, ten krążek ma praktycznie taki sam mały rozmiar niezależnie od tego, czy obiekt jest blisko, czy daleko.\nSkoro każdy obiekt, bez względu na jego odległość, jest rzutowany na ekran z podobnie małym stopniem \u0026ldquo;rozmycia\u0026rdquo; (który nasze oko interpretuje jako punkt), cały obraz od pierwszego planu po horyzont wydaje się jednakowo ostry.\nW skrócie: W aparacie z obiektywem ostrość to stan, w którym soczewka aktywnie skupia światło w punkcie. W Camera Obscura nie ma aktywnego skupiania, więc nie ma też pojęcia \u0026ldquo;nieostrości\u0026rdquo;. Zamiast tego mamy uniwersalną, choć nie idealną, ostrość dla całej sceny.\n3. Kompromis: Ostrość a Dyfrakcja # Czy to oznacza, że Camera Obscura jest optycznie doskonała? Nie. Jej \u0026ldquo;nieskończona głębia ostrości\u0026rdquo; ma swoją cenę.\nOstrość vs. Rozdzielczość: Obraz w Camera Obscura jest \u0026ldquo;w pełni w focusie\u0026rdquo;, ale nigdy nie będzie tak krystalicznie ostry i szczegółowy jak obraz z dobrego obiektywu. Jego ostrość jest ograniczona rozmiarem otworu. Limit Dyfrakcji: Jeśli spróbujemy uzyskać idealną ostrość, zmniejszając otwór do absolutnego minimum, napotkamy barierę fizyczną zwaną dyfrakcją. Gdy otwór staje się ekstremalnie mały (porównywalny z długością fali świetlnej), światło zaczyna się na jego krawędziach uginać i rozpraszać. Paradoksalnie, powoduje to, że obraz znów staje się bardziej miękki i traci na szczegółowości. Dlatego nawet w idealnej Camera Obscura istnieje optymalny rozmiar otworu, który jest kompromisem między minimalizacją \u0026ldquo;krążka rozmycia\u0026rdquo; a uniknięciem negatywnych efektów dyfrakcji.\nCamera Obscura ma więc nieskończoną głębię ostrości nie dlatego, że posiada jakiś zaawansowany mechanizm, ale wręcz przeciwnie – dlatego, że jest go pozbawiona. Jej prostota jest źródłem tej fascynującej właściwości.\nCo Decyduje o \u0026ldquo;Ogniskowej\u0026rdquo; w Camera Obscura? Odległość, Nie Ognisko # Pytanie o ogniskową w Camera Obscura jest niezwykle trafne, ponieważ dotyka fundamentalnej różnicy między tym urządzeniem a aparatem wyposażonym w obiektyw. W ścisłym, optycznym sensie, Camera Obscura nie posiada ogniskowej, ponieważ nie ma soczewki, a co za tym idzie – nie ma punktu ogniskowego, w którym zbiegają się promienie światła.\nJednakże, istnieje parametr, który pełni w Camera Obscura rolę analogiczną do ogniskowej w aparacie z obiektywem. Tym parametrem jest odległość między otworem (pinhole) a płaszczyzną obrazu (ekranem).\nTo właśnie ta odległość decyduje o dwóch kluczowych cechach obrazu, tak jak robi to ogniskowa w obiektywie: o kącie widzenia i powiększeniu.\nJak Odległość Otworu od Ekranu Wpływa na Obraz? # Wyobraźmy sobie naszą Camera Obscura jako proste pudełko. Odległość, o której mówimy, to po prostu jego głębokość.\n1. Krótka \u0026ldquo;Ogniskowa\u0026rdquo; (Mała odległość otworu od ekranu) # Działanie: Gdy ekran znajduje się blisko otworu, do \u0026ldquo;złapania\u0026rdquo; obrazu wykorzystywany jest bardzo szeroki stożek światła wpadającego do środka. Efekt: Otrzymujemy obraz o szerokim kącie widzenia. Na ekranie mieści się bardzo rozległy fragment sceny. Obiekty wydają się małe, a perspektywa jest przerysowana – podobnie jak w przypadku obiektywów szerokokątnych w tradycyjnej fotografii. 2. Długa \u0026ldquo;Ogniskowa\u0026rdquo; (Duża odległość otworu od ekranu) # Działanie: Gdy odsuniemy ekran daleko od otworu, obejmie on znacznie węższy fragment stożka wpadającego światła. Efekt: Otrzymujemy obraz o wąskim kącie widzenia i większym powiększeniu. Na ekranie widzimy tylko niewielki wycinek sceny, ale za to jest on znacznie powiększony. Działa to analogicznie do teleobiektywu. Podsumowanie: Odpowiednik Ogniskowej # Można więc powiedzieć, że w Camera Obscura \u0026ldquo;ogniskowa\u0026rdquo; jest pojęciem umownym i jest bezpośrednio równa fizycznej odległości od otworu do powierzchni, na której rzutowany jest obraz.\nW przeciwieństwie do aparatu z obiektywem, gdzie zmiana ogniskowej (zoom) to skomplikowany ruch soczewek, w Camera Obscura \u0026ldquo;zmiana ogniskowej\u0026rdquo; sprowadzałaby się do fizycznego przesunięcia tylnej ścianki – tak jak w starych aparatach miechowych. Prostota tego mechanizmu po raz kolejony ukazuje fundamentalną naturę optyki leżącej u podstaw tego fascynującego urządzenia.\n"},{"id":2,"href":"/photography-basics/docs/soczewka/","title":"Soczewka","section":"Docs","content":" Soczewka # Obiektyw, będący sercem każdego aparatu fotograficznego, to precyzyjny instrument optyczny, którego zadaniem jest zebranie światła z otaczającej sceny i uformowanie z niego ostrego obrazu na powierzchni materiału światłoczułego. Zrozumienie jego działania wymaga zagłębienia się w podstawy optyki i poznania kluczowych parametrów, takich jak ogniskowa.\nPodstawy optyki: załamanie światła i działanie soczewek # Fundamentalnym zjawiskiem, na którym opiera się działanie soczewek, jest załamanie światła (refrakcja). Dochodzi do niego, gdy promień świetlny przechodzi przez granicę dwóch przezroczystych ośrodków różniących się gęstością optyczną (np. z powietrza do szkła lub ze szkła do powietrza).3 Zmiana gęstości optycznej powoduje zmianę prędkości rozchodzenia się światła, co z kolei prowadzi do zmiany kierunku jego biegu, o ile promień nie pada prostopadle na granicę ośrodków. Wielkość tego odchylenia opisuje prawo Snella.\nSoczewka to bryła wykonana z przezroczystego materiału (najczęściej szkła optycznego, ale także specjalnych tworzyw sztucznych), posiadająca co najmniej jedną powierzchnię krzywą (zazwyczaj sferyczną, cylindryczną lub bardziej złożoną – asferyczną).2 Dzięki odpowiedniemu ukształtowaniu tych powierzchni, soczewka jest w stanie w kontrolowany sposób załamywać przechodzące przez nią promienie świetlne.\nWyróżniamy dwa podstawowe typy soczewek:\nSoczewki skupiające (wypukłe): Charakteryzują się tym, że są grubsze w części środkowej niż na brzegach. Ich główną właściwością jest zdolność do skupiania wiązki promieni równoległych (np. pochodzących od bardzo odległego obiektu) w jednym punkcie, zwanym ogniskiem (F).2 To właśnie soczewki skupiające są odpowiedzialne za tworzenie rzeczywistego, odwróconego obrazu na matrycy lub kliszy fotograficznej. Soczewki rozpraszające (wklęsłe): Są cieńsze w środku i grubsze na brzegach. Powodują one rozbieganie się wiązki promieni równoległych w taki sposób, jakby wychodziły one z jednego punktu położonego przed soczewką – jest to tzw. ognisko pozorne.2 Należy podkreślić, że obiektyw fotograficzny to nie pojedyncza soczewka, lecz złożony układ optyczny, składający się z wielu (czasem nawet kilkunastu lub kilkudziesięciu) soczewek, zarówno skupiających, jak i rozpraszających. Taka konstrukcja jest niezbędna do skorygowania licznych wad optycznych (aberracji), które nieuchronnie towarzyszą pojedynczym soczewkom, oraz do zapewnienia wysokiej jakości obrazu w całym kadrze. W nowoczesnych obiektywach stosuje się soczewki o specjalnych kształtach, np. asferyczne, które minimalizują zniekształcenia geometryczne i poprawiają ostrość na brzegach obrazu 2, oraz soczewki wykonane ze specjalnych gatunków szkła, np. niskodyspersyjnego (ED, UD, LD, SLD), które redukują aberrację chromatyczną (rozszczepienie światła na barwy składowe, prowadzące do kolorowych obwódek na kontrastowych krawędziach).5 Zrozumienie, że obiektyw jest tak skomplikowanym systemem, pozwala docenić wysiłek inżynierów w dążeniu do jak najwierniejszego przeniesienia trójwymiarowej rzeczywistości na płaską powierzchnię rejestrującą.\nPojęcie ogniskowej (Focal Length) # Ogniskowa (F) jest jednym z najważniejszych parametrów charakteryzujących obiektyw. Z fizycznego punktu widzenia, jest to odległość od środka optycznego obiektywu (lub, precyzyjniej, od jego głównej płaszczyzny obrazowej tylnej) do punktu, w którym skupiają się promienie światła wpadające do obiektywu równolegle do jego osi optycznej.6 Takie równoległe promienie pochodzą od obiektów znajdujących się w bardzo dużej odległości (teoretycznie w nieskończoności). W tym punkcie, zwanym ogniskiem, powstaje ostry obraz tych odległych obiektów. Ogniskową wyraża się zazwyczaj w milimetrach (mm).\nWarto zaznaczyć, że pojęcie \u0026ldquo;środka optycznego\u0026rdquo; jest pewnym uproszczeniem dla cienkich soczewek. W rzeczywistych, grubych obiektywach fotograficznych, które są układami wielu soczewek, mówi się o tzw. płaszczyznach głównych (przedmiotowej i obrazowej), i to odległość od płaszczyzny głównej obrazowej do ogniska definiuje ogniskową.\nOkreślenia \u0026ldquo;długa ogniskowa\u0026rdquo; i \u0026ldquo;krótka ogniskowa\u0026rdquo; odnoszą się bezpośrednio do tej odległości:\nDługa ogniskowa oznacza, że punkt skupienia (ognisko) znajduje się stosunkowo daleko za układem soczewek. Krótka ogniskowa oznacza, że ognisko leży bliżej układu soczewek.7 Ogniskowa jest fundamentalnym parametrem, który determinuje, jak obiektyw \u0026ldquo;widzi\u0026rdquo; świat i jak odwzorowuje go na materiale światłoczułym.\nWpływ ogniskowej na obraz # Zmiana ogniskowej obiektywu pociąga za sobą szereg konsekwencji dla rejestrowanego obrazu, wpływając na jego kompozycję, odbiór i charakter.\nKąt widzenia (Angle of View): Jest to najważniejsza cecha wynikająca bezpośrednio z ogniskowej. Kąt widzenia określa, jak szeroki fragment sceny jest w stanie \u0026ldquo;zobaczyć\u0026rdquo; obiektyw i zarejestrować na materiale światłoczułym. Krótka ogniskowa (np. 16mm, 24mm na pełnej klatce) daje szeroki kąt widzenia. Obiektyw obejmuje duży obszar sceny, co jest przydatne w fotografii krajobrazowej, architekturze czy w ciasnych pomieszczeniach.6 Długa ogniskowa (np. 200mm, 400mm na pełnej klatce) daje wąski kąt widzenia. Obiektyw \u0026ldquo;wycina\u0026rdquo; niewielki fragment sceny, sprawiając wrażenie jego przybliżenia. Jest to charakterystyczne dla teleobiektywów używanych w fotografii sportowej czy przyrodniczej.6 Aby zilustrować tę zależność, można sobie wyobrazić patrzenie na świat przez tubę – im dłuższa tuba (dłuższa ogniskowa), tym mniejszy wycinek rzeczywistości widzimy. Powiększenie obrazu (Magnification): Dłuższe ogniskowe dają większe powiększenie obiektów w kadrze.6 Obiekty wydają się być większe i znajdujące się bliżej, niż są w rzeczywistości. Krótsze ogniskowe dają mniejsze powiększenie, przez co obiekty są mniejsze na zdjęciu. Perspektywa (Perspective): To, jak postrzegamy relacje przestrzenne między obiektami na zdjęciu, jest ściśle związane z wyborem ogniskowej. Należy jednak podkreślić, że ogniskowa sama w sobie nie zmienia perspektywy geometrycznej – ta zależy wyłącznie od położenia aparatu (punktu widzenia) względem fotografowanej sceny. Ogniskowa wpływa natomiast na to, jak ta perspektywa jest odwzorowana na płaskim obrazie i jakie wrażenie przestrzenne odbieramy. Krótkie ogniskowe (obiektywy szerokokątne): Mają tendencję do \u0026ldquo;rozciągania\u0026rdquo; przestrzeni. Obiekty znajdujące się na pierwszym planie wydają się nieproporcjonalnie duże w stosunku do obiektów w tle, które z kolei wydają się mniejsze i bardziej oddalone – jest to efekt tzw. \u0026ldquo;uciekania tła\u0026rdquo;.6 Taka reprodukcja perspektywy może dodawać zdjęciom dynamiki i wrażenia głębi. Należy jednak uważać na możliwość wystąpienia dystorsji geometrycznych, szczególnie przy fotografowaniu obiektów blisko krawędzi kadru lub bardzo blisko obiektywu (np. przerysowanie twarzy w portrecie).6 Długie ogniskowe (teleobiektywy): Powodują tzw. \u0026ldquo;kompresję perspektywy\u0026rdquo;. Oznacza to, że odległości między obiektami znajdującymi się na różnych planach wydają się być optycznie zmniejszone – obiekty wyglądają na bliższe siebie, niż są w rzeczywistości.6 Tło wydaje się być \u0026ldquo;przyciągnięte\u0026rdquo; do głównego motywu, a cała scena sprawia wrażenie bardziej spłaszczonej. Ogniskowa standardowa (około 50 mm dla aparatu pełnoklatkowego): Uważana jest za dającą perspektywę najbardziej zbliżoną do sposobu, w jaki postrzega świat ludzkie oko.6 Obiekty i relacje przestrzenne są odwzorowywane w sposób naturalny. Wybór ogniskowej jest zatem nie tylko kwestią tego, \u0026ldquo;ile się zmieści w kadrze\u0026rdquo;, ale przede wszystkim świadomym narzędziem kształtowania narracji wizualnej i wpływania na odbiór przestrzeni przez widza. Głębia ostrości (Depth of Field - DOF): Ogniskowa jest jednym z trzech głównych czynników (obok wartości przysłony i odległości od fotografowanego obiektu) wpływających na głębię ostrości, czyli zakres odległości, w którym obiekty na zdjęciu wydają się ostre. Przy tej samej wartości przysłony i tej samej odległości od obiektu, dłuższe ogniskowe dają mniejszą (płytszą) głębię ostrości. Oznacza to, że łatwiej jest uzyskać efekt rozmytego tła (bokeh), izolując główny obiekt od otoczenia.6 Krótsze ogniskowe dają większą (głębszą) głębię ostrości. Trudniej jest uzyskać mocne rozmycie tła, a więcej elementów w kadrze, zarówno na pierwszym planie, jak i w tle, pozostaje ostrych.7 Szczegółowe omówienie mechanizmu powstawania głębi ostrości i roli przysłony znajduje się w dalszej części prezentacji, jednak już tutaj należy zaznaczyć istotny wpływ ogniskowej na ten aspekt obrazu. Rodzaje obiektywów ze względu na ogniskową i ich zastosowania # Różnorodność dostępnych ogniskowych pozwala fotografom na dobór odpowiedniego narzędzia do konkretnego zadania i zamierzonego efektu artystycznego. Obiektywy można podzielić na kilka głównych kategorii (podane zakresy ogniskowych odnoszą się do aparatów pełnoklatkowych, dla matryc mniejszych formatów należy uwzględnić tzw. mnożnik ogniskowej):\nRybie oko (Fisheye): Ogniskowe zazwyczaj w zakresie 6-16 mm.5 Charakteryzują się ekstremalnie szerokim kątem widzenia, często obejmującym 180 stopni lub więcej, oraz bardzo silną, charakterystyczną dystorsją beczkowatą, która zakrzywia linie proste (poza tymi przechodzącymi przez środek kadru). Obraz może być kołowy lub wypełniać cały kadr. Zastosowanie: Tworzenie efektów specjalnych, fotografowanie panoram sferycznych, sportów ekstremalnych, bardzo ciasnych wnętrz. Obiektywy szerokokątne (Wide-angle): Ogniskowe typowo od około 10-14 mm (ultraszerokokątne) do 35 mm.6 Oferują szeroki kąt widzenia, pozwalają na uchwycenie rozległych scen, podkreślenie przestrzeni i dynamiki. Zastosowanie: Fotografia krajobrazowa, architektury (zarówno zewnętrznej, jak i wnętrz), reportaż (szczególnie w tłumie lub ograniczonych przestrzeniach), astrofotografia (np. Droga Mleczna). Należy pamiętać o możliwych zniekształceniach perspektywicznych przy fotografowaniu ludzi z bliska. Obiektywy standardowe (Standard): Ogniskowe w zakresie około 35-55 mm, najczęściej przyjmuje się 50 mm jako standard dla pełnej klatki.6 Kąt widzenia i odwzorowanie perspektywy tych obiektywów są uważane za najbardziej zbliżone do naturalnego postrzegania świata przez ludzkie oko. Zastosowanie: Fotografia uliczna, reportaż, portrety środowiskowe (pokazujące osobę w jej otoczeniu), fotografia dokumentalna, codzienna fotografia (\u0026ldquo;do wszystkiego\u0026rdquo;). Teleobiektywy (Telephoto): Krótkie teleobiektywy (Short telephoto): Ogniskowe od około 70 mm do 135 mm.6 Zapewniają węższy kąt widzenia niż standardowe, co pozwala na pewne \u0026ldquo;przybliżenie\u0026rdquo; obiektu i jego izolację od tła. Dają przyjemną kompresję perspektywy i umożliwiają uzyskanie płytkiej głębi ostrości. Zastosowanie: Idealne do fotografii portretowej (np. ogniskowe 85mm, 100mm, 135mm są cenione za naturalne odwzorowanie proporcji twarzy i piękne rozmycie tła – bokeh), fotografia mody, detali, sport z nieco bliższej odległości. Długie teleobiektywy i superteleobiektywy (Long telephoto / Supertelephoto): Ogniskowe od około 200 mm wzwyż, sięgające 600 mm, 800 mm, a nawet więcej.6 Charakteryzują się bardzo wąskim kątem widzenia, silnym powiększeniem odległych obiektów, mocną kompresją perspektywy i bardzo płytką głębią ostrości. Zastosowanie: Fotografia sportowa (szczególnie na dużych arenach), dzikiej przyrody (ptaki, zwierzęta z dużej odległości), astrofotografia (np. Księżyc, Słońce z odpowiednimi filtrami), fotografia lotnicza. Obiektywy makro (Macro): Są to specjalnie skonstruowane obiektywy, które pozwalają na uzyskanie bardzo dużej skali odwzorowania (często 1:1, co oznacza, że obraz obiektu na matrycy ma takie same wymiary jak sam obiekt, lub nawet większej). Umożliwiają fotografowanie bardzo małych obiektów z niewielkiej odległości, ukazując detale niewidoczne gołym okiem. Ogniskowe obiektywów makro są różne, często spotykane to 50mm, 60mm, 90mm, 100mm, 105mm, 150mm, 180mm.6 Zastosowanie: Fotografia przyrodnicza (owady, kwiaty, detale roślin), fotografia produktowa (małe przedmioty, biżuteria), fotografia techniczna i naukowa. Obiektywy stałoogniskowe (Prime Lenses) vs. zmiennoogniskowe (Zoom Lenses) # Fotografowie stają często przed wyborem między obiektywami o stałej ogniskowej a obiektywami zmiennoogniskowymi. Każdy typ ma swoje zalety i wady.\nObiektywy stałoogniskowe (tzw. \u0026ldquo;stałki\u0026rdquo;, prime lenses): Posiadają jedną, niezmienną wartość ogniskowej.6 Aby zmienić kadrowanie (powiększenie obiektu), fotograf musi fizycznie zmienić swoją odległość od fotografowanej sceny. Zalety: Lepsza jakość optyczna: Zazwyczaj oferują wyższą jakość obrazu. Ich prostsza konstrukcja optyczna (mniejsza liczba grup soczewek i pojedynczych soczewek) pozwala na lepszą korekcję wad optycznych, co przekłada się na większą ostrość, lepszy kontrast i mniejsze zniekształcenia.6 Większa jasność (światłosiła): Często posiadają znacznie większy maksymalny otwór względny przysłony (np. f/1.8, f/1.4, a nawet f/1.2 czy f/0.) w porównaniu do obiektywów zmiennoogniskowych w podobnej cenie.6 Pozwala to na fotografowanie w trudniejszych warunkach oświetleniowych bez konieczności podnoszenia czułości ISO oraz na uzyskanie bardzo płytkiej głębi ostrości i pięknego efektu bokeh. Mniejsze wymiary i waga: Zazwyczaj są mniejsze i lżejsze od zoomów o porównywalnej jakości. Wymuszają kreatywność: Brak możliwości \u0026ldquo;zoomowania\u0026rdquo; zmusza fotografa do bardziej świadomego kadrowania, aktywnego poszukiwania najlepszego punktu widzenia i przemieszczania się, co może prowadzić do ciekawszych ujęć. Wady: Brak wszechstronności: Konieczność zmiany obiektywu (jeśli posiadamy ich kilka) lub fizycznego przemieszczania się w celu zmiany kadru może być niewygodna i czasochłonna, szczególnie w dynamicznych sytuacjach (np. reportaż, fotografia sportowa). Aby pokryć szeroki zakres zastosowań, często potrzebne jest posiadanie kilku obiektywów stałoogniskowych. Obiektywy zmiennoogniskowe (tzw. \u0026ldquo;zoomy\u0026rdquo;, zoom lenses): Umożliwiają płynną zmianę ogniskowej w określonym zakresie (np. 24-70mm, 70-200mm) za pomocą pierścienia na obudowie obiektywu. Zalety: Wszechstronność i wygoda: Główną zaletą jest możliwość szybkiej zmiany kadru i powiększenia obiektu bez konieczności zmiany obiektywu czy zmiany pozycji fotografa. Jeden obiektyw zmiennoogniskowy może zastąpić kilka obiektywów stałoogniskowych, co jest szczególnie przydatne w podróży, fotografii reportażowej czy eventowej. Szybkość reakcji: Pozwalają na błyskawiczne dostosowanie kadru do zmieniającej się sytuacji. Wady: Jakość optyczna: Ze względu na bardziej skomplikowaną konstrukcję (większa liczba ruchomych grup soczewek), osiągnięcie takiej samej jakości optycznej jak w przypadku dobrych \u0026ldquo;stałek\u0026rdquo; jest trudniejsze i często droższe. Tańsze zoomy mogą wykazywać większe dystorsje, winietowanie czy spadek ostrości na brzegach kadru, zwłaszcza przy skrajnych ogniskowych. Jasność (światłosiła): Wiele popularnych i tańszych zoomów ma zmienną maksymalną przysłonę, która staje się \u0026ldquo;ciemniejsza\u0026rdquo; (mniejszy otwór, większa liczba f) wraz ze wzrostem ogniskowej (np. f/3.5-5.6). Profesjonalne zoomy o stałej, dużej jasności (np. f/2.8) są zazwyczaj znacznie droższe, większe i cięższe. Rozmiar i waga: Zazwyczaj są większe i cięższe od obiektywów stałoogniskowych o podobnym zakresie najkrótszej ogniskowej. Cena: Wysokiej jakości obiektywy zmiennoogniskowe, zwłaszcza te o stałym świetle, są zazwyczaj drogie. Wybór między obiektywem stałoogniskowym a zmiennoogniskowym jest często kompromisem między maksymalną jakością optyczną i światłosiłą (charakterystyczną dla \u0026ldquo;stałek\u0026rdquo;) a wszechstronnością i wygodą (oferowaną przez \u0026ldquo;zoomy\u0026rdquo;). Jednakże, postęp technologiczny w dziedzinie optyki sprawia, że najlepsze współczesne obiektywy zmiennoogniskowe coraz bardziej zbliżają się jakością obrazu do obiektywów stałoogniskowych, choć zazwyczaj wiąże się to z wyższą ceną. Dla wielu profesjonalistów wysokiej klasy zoomy oferują wystarczającą jakość przy znacznie większej elastyczności. Dla amatorów i entuzjastów, jasne obiektywy stałoogniskowe mogą być bardziej dostępne cenowo i oferować lepszą jakość obrazu oraz większą światłosiłę niż podstawowe, tzw. \u0026ldquo;kitowe\u0026rdquo; obiektywy zmiennoogniskowe.\nTabela: Porównanie Typowych Ogniskowych (dla pełnej klatki) i Ich Zastosowań # Poniższa tabela przedstawia skondensowane informacje dotyczące praktycznych implikacji wyboru różnych ogniskowych w aparatach pełnoklatkowych. Łączy ona teoretyczną wiedzę o kącie widzenia i wpływie na perspektywę z konkretnymi przykładami zastosowań, co ułatwia przełożenie teorii na praktykę fotograficzną.\nKategoria Obiektywu Zakres Ogniskowych (FF) Przybliżony Kąt Widzenia (po przekątnej) Charakterystyczny Wpływ na Obraz Główne Zastosowania Rybie Oko 6-16 mm 180° - 220° Silna dystorsja beczkowata, zakrzywienie linii prostych Efekty artystyczne, panoramy sferyczne, sporty ekstremalne Ultraszerokokątny 14-24 mm 114° - 84° Mocne rozciągnięcie perspektywy, wrażenie przestrzeni, możliwa dystorsja Architektura, wnętrza, rozległe krajobrazy, astrofotografia Szerokokątny 24-35 mm 84° - 63° Umiarkowane rozciągnięcie perspektywy, duża głębia ostrości Krajobraz, reportaż, fotografia uliczna, grupy ludzi Standardowy (35) 50 (55) mm 63° - 47° Naturalna perspektywa, zbliżona do ludzkiego oka Fotografia codzienna, portret środowiskowy, reportaż Krótki Teleobiektyw 70-135 mm 34° - 18° Lekka kompresja perspektywy, izolacja obiektu, płytka głębia ostrości Portrety (szczególnie 85mm, 105mm, 135mm), moda, detale Średni Teleobiektyw 135-300 mm 18° - 8° Wyraźna kompresja perspektywy, silna izolacja obiektu Sport, dzika przyroda (bliższe dystanse), portrety z daleka Długi Teleobiektyw (Supertele) 300 mm + \u0026lt; 8° Ekstremalna kompresja, duże przybliżenie odległych obiektów Sport (duże obiekty), dzika przyroda, fotografia lotnicza Wybór ogniskowej to fundamentalna decyzja artystyczna, a nie tylko techniczna. Kształtuje ona sposób, w jaki widz postrzega przestrzeń i relacje między obiektami na zdjęciu, stając się narzędziem do opowiadania historii. Różne ogniskowe dają różne kąty widzenia, co determinuje, ile sceny jest uchwycone.6 Co ważniejsze, ogniskowa wpływa na odwzorowanie perspektywy – krótkie ogniskowe \u0026ldquo;rozciągają\u0026rdquo; przestrzeń i oddalają tło, a długie \u0026ldquo;kompresują\u0026rdquo; ją i przybliżają tło.6 Manipulacja perspektywą pozwala fotografowi podkreślić pewne elementy, ukryć inne, stworzyć wrażenie głębi lub spłaszczenia, izolować obiekt lub pokazać go w kontekście.6 Te wybory wpływają na odbiór emocjonalny zdjęcia i narrację wizualną.7\nIstnieje również nierozerwalny związek między ogniskową, odległością od obiektu a głębią ostrości. Te trzy elementy muszą być rozpatrywane łącznie przy planowaniu zdjęcia. Zmiana jednego pociąga za sobą konieczność dostosowania pozostałych, jeśli chcemy osiągnąć określony efekt. Dłuższe ogniskowe generalnie dają mniejszą głębię ostrości 6, a mniejsza odległość od obiektu również skutkuje mniejszą głębią ostrości. Jeśli fotograf chce uzyskać ten sam kadr (to samo powiększenie obiektu) przy użyciu różnych ogniskowych, musi zmienić swoją odległość od obiektu. Ta zmiana odległości również wpłynie na głębię ostrości. Jednak dominujący efekt na głębię ostrości (przy zachowaniu tego samego kadru) ma zmiana ogniskowej – dłuższe ogniskowe ułatwiają uzyskanie płytszej głębi. Zrozumienie tych współzależności jest kluczowe dla precyzyjnej kontroli nad ostrością i rozmyciem w kadrze.\n"},{"id":3,"href":"/photography-basics/docs/migawka/","title":"Migawka","section":"Docs","content":" Migawka # Migawka jest jednym z fundamentalnych elementów aparatu fotograficznego, pełniącym kluczową rolę w procesie naświetlania. Jej precyzyjne działanie decyduje o tym, jak długo światło będzie oddziaływać na materiał światłoczuły, co ma bezpośredni wpływ na jasność zdjęcia oraz sposób odwzorowania ruchu.\nRola migawki w aparacie fotograficznym # Główną i najważniejszą funkcją migawki jest precyzyjne kontrolowanie czasu, przez który materiał światłoczuły (klisza w aparacie analogowym lub matryca w aparacie cyfrowym) jest wystawiony na działanie światła wpadającego przez obiektyw. Ten interwał czasowy nazywany jest czasem naświetlania lub czasem ekspozycji. W stanie spoczynku, gdy nie wykonujemy zdjęcia, migawka pozostaje zamknięta, chroniąc materiał światłoczuły przed niepożądanym naświetleniem. Dopiero naciśnięcie spustu migawki (lub odpowiedniego przycisku w aparacie) inicjuje sekwencję zdarzeń: migawka otwiera się, przepuszczając światło na określony czas, a następnie zamyka się, kończąc proces naświetlania. Czas naświetlania, obok wartości przysłony i czułości ISO, jest jednym z trzech filarów prawidłowej ekspozycji zdjęcia. Co więcej, czas otwarcia migawki ma decydujący wpływ na sposób rejestracji ruchu na zdjęciu: krótkie czasy pozwalają \u0026ldquo;zamrozić\u0026rdquo; dynamiczne sceny, podczas gdy długie czasy prowadzą do efektu rozmycia poruszających się obiektów, co może być wykorzystywane w celach artystycznych.\nMigawka szczelinowa (kurtynowa) – budowa i zasada działania # Migawka szczelinowa, nazywana również kurtynową, jest najczęściej spotykanym typem migawki w aparatach systemowych, takich jak lustrzanki (SLR i DSLR) oraz nowoczesne aparaty bezlusterkowe. Umieszczona jest ona bezpośrednio przed płaszczyzną materiału światłoczułego, w korpusie aparatu.\nBudowa: Konstrukcja migawki szczelinowej opiera się na dwóch (lub czasami więcej) elastycznych kurtynkach wykonanych z impregnowanej tkaniny lub, częściej w nowoczesnych aparatach, z zestawu cienkich, metalowych lamelek (blaszek). Te kurtynki lub lamelki przesuwają się wzdłuż powierzchni materiału światłoczułego. W zależności od konstrukcji, ruch ten może odbywać się w kierunku poziomym lub pionowym. W większości współczesnych aparatów stosuje się migawki o ruchu pionowym, ponieważ krótsza droga, jaką muszą pokonać kurtynki (wzdłuż krótszego boku klatki), pozwala na osiągnięcie krótszych minimalnych czasów naświetlania oraz krótszych czasów synchronizacji z lampą błyskową.\nZasada działania: Sposób działania migawki szczelinowej różni się w zależności od ustawionego czasu naświetlania:\nDłuższe czasy naświetlania (np. od najdłuższych dostępnych do około 1/60s - 1/250s): Wartość graniczna zależy od konkretnego modelu aparatu i jest ściśle związana z tzw. czasem synchronizacji z lampą błyskową (X-sync). Przy tych czasach, po naciśnięciu spustu, pierwsza kurtynka (lub zestaw lamelek) rozpoczyna ruch, całkowicie odsłaniając całą powierzchnię materiału światłoczułego. Materiał pozostaje odsłonięty przez zadany czas naświetlania. Po upływie tego czasu, druga kurtynka rozpoczyna swój ruch, zasłaniając materiał światłoczuły i kończąc ekspozycję. W tym trybie pracy cała powierzchnia sensora jest naświetlana jednocześnie przez cały czas otwarcia migawki. Krótsze czasy naświetlania (krótsze niż czas X-sync): Gdy wymagany czas ekspozycji jest bardzo krótki, mechanizm działa inaczej. Druga kurtynka zaczyna swój ruch zasłaniający, zanim pierwsza kurtynka zdąży całkowicie odsłonić materiał światłoczuły. W efekcie, między obiema kurtynami tworzy się wąska szczelina o stałej szerokości, która przesuwa się wzdłuż powierzchni materiału światłoczułego, naświetlając kolejno jego fragmenty. Im krótszy jest ustawiony czas naświetlania, tym węższa jest ta szczelina. Mimo że każdy fragment materiału światłoczułego jest naświetlany przez bardzo krótki czas (równy czasowi przejścia szczeliny nad danym punktem), cały proces przejścia szczeliny przez całą klatkę trwa dłużej (jest to czas X-sync). Zrozumienie tego dwutrybowego działania migawki szczelinowej jest absolutnie kluczowe dla pojęcia czasu synchronizacji z lampą błyskową oraz dla zrozumienia niektórych anomalii, takich jak efekt \u0026ldquo;rolling shutter\u0026rdquo; w przypadku bardzo szybkiego ruchu obiektu względem ruchu szczeliny.\nInne rodzaje migawek (jako ciekawostki i uzupełnienie wiedzy) # Oprócz dominującej migawki szczelinowej, w historii fotografii oraz w niektórych specjalistycznych zastosowaniach pojawiały się i nadal są używane inne typy migawek.\nMigawka centralna (irysowa, listkowa): Budowa i umiejscowienie: Ten typ migawki składa się z kilku (zazwyczaj od 3 do 7) cienkich, metalowych listków (blaszek) o sierpowatym kształcie, które zachodzą na siebie, tworząc otwór. Jest ona umieszczona wewnątrz obiektywu, najczęściej w pobliżu płaszczyzny przysłony irysowej. Zasada działania: Po wyzwoleniu, listki migawki centralnej rozchylają się promieniście od środka na zewnątrz, odsłaniając cały otwór obiektywu, a następnie w podobny sposób zamykają się. Istotne jest to, że cała powierzchnia materiału światłoczułego jest naświetlana jednocześnie przez cały czas otwarcia migawki, niezależnie od ustawionego czasu ekspozycji. Zalety: Główną zaletą migawki centralnej jest możliwość synchronizacji z lampą błyskową przy wszystkich dostępnych czasach naświetlania. Ponieważ cała klatka jest odsłaniana jednocześnie, krótki błysk lampy może ją równomiernie oświetlić w dowolnym momencie trwania ekspozycji. Jest to niezwykle cenne w fotografii studyjnej, gdzie często pracuje się z lampami błyskowymi. Dodatkowo, migawki centralne są zazwyczaj cichsze w działaniu i generują mniej wibracji niż migawki szczelinowe. Wady: Konstrukcja obiektywu z wbudowaną migawką centralną jest bardziej skomplikowana i zazwyczaj droższa. Trudniej jest również w tego typu migawkach zrealizować bardzo krótkie czasy naświetlania – typowe minimalne czasy to 1/500s, rzadziej 1/1000s czy 1/2000s. Zastosowanie: Tradycyjnie stosowana w aparatach średnioformatowych (np. Hasselblad, Rolleiflex z obiektywami z migawką centralną), niektórych aparatach kompaktowych wyższej klasy oraz w obiektywach do aparatów wielkoformatowych. Migawka elektroniczna: Zasada działania: W odróżnieniu od migawek mechanicznych, migawka elektroniczna nie jest osobnym, ruchomym elementem aparatu, lecz funkcją realizowaną przez samą matrycę światłoczułą (najczęściej typu CMOS). Kontrola czasu ekspozycji odbywa się poprzez elektroniczne sterowanie procesem zbierania ładunku przez poszczególne piksele (lub linie pikseli) matrycy. Naświetlanie rozpoczyna się od zresetowania pikseli, a kończy poprzez odczytanie zgromadzonego przez nie ładunku po upływie zadanego czasu. Zalety: Całkowicie bezgłośna praca: Brak ruchomych części mechanicznych oznacza brak jakiegokolwiek dźwięku podczas robienia zdjęcia. Brak drgań: Eliminacja drgań mechanicznych (tzw. \u0026ldquo;shutter shock\u0026rdquo;) jest szczególnie ważna przy fotografowaniu z długimi ogniskowymi, w makrofotografii lub przy długich czasach naświetlania bez statywu. Bardzo krótkie czasy naświetlania: Migawki elektroniczne pozwalają na osiąganie ekstremalnie krótkich czasów ekspozycji, np. 1/16000s, 1/32000s, a nawet krótszych. Bardzo wysokie prędkości zdjęć seryjnych: Brak ograniczeń mechanicznych pozwala na rejestrowanie wielu klatek na sekundę. Wady: Efekt \u0026ldquo;rolling shutter\u0026rdquo;: Wiele matryc z migawką elektroniczną odczytuje obraz nie z całej powierzchni jednocześnie, lecz linia po linii (lub kolumna po kolumnie) z pewnym opóźnieniem. Może to prowadzić do zniekształceń szybko poruszających się obiektów (opisane dalej). Problemy z migotaniem sztucznego oświetlenia (banding): Szybki odczyt linii pikseli może powodować powstawanie poziomych pasów na zdjęciach wykonanych przy oświetleniu jarzeniowym lub LED, które migocze z częstotliwością sieci energetycznej. Ograniczona dynamika tonalna lub głębia bitowa: W niektórych implementacjach, szczególnie przy bardzo krótkich czasach, jakość obrazu (np. zakres tonalny) może być nieco niższa niż przy użyciu migawki mechanicznej. Synchronizacja z lampą błyskową: Czasem synchronizacja z lampą błyskową jest wolniejsza niż w przypadku migawek mechanicznych, lub tryb HSS może nie być dostępny lub działać inaczej. Zastosowanie: Coraz powszechniejsza w nowoczesnych aparatach bezlusterkowych, smartfonach, kamerach sportowych i profesjonalnych kamerach wideo. Niektóre zaawansowane matryce posiadają tzw. global shutter (migawkę globalną), gdzie wszystkie piksele rozpoczynają i kończą naświetlanie dokładnie w tym samym momencie, eliminując efekt rolling shutter, ale są to rozwiązania droższe. Migawka hybrydowa (np. elektroniczna pierwsza kurtynka - EFCS): Jest to rozwiązanie łączące cechy migawki mechanicznej i elektronicznej. Popularnym przykładem jest EFCS (Electronic Front-Curtain Shutter), gdzie ekspozycja rozpoczyna się elektronicznie (podobnie jak w migawce elektronicznej, pierwsza kurtynka jest symulowana przez matrycę), a kończy się za pomocą tradycyjnej, mechanicznej drugiej kurtynki. Zalety: Główną zaletą jest redukcja drgań powodowanych przez ruch pierwszej kurtynki mechanicznej, co jest szczególnie korzystne przy czasach naświetlania podatnych na poruszenia (np. 1/30s - 1/250s) i przy użyciu teleobiektywów. Praca jest również cichsza niż w przypadku w pełni mechanicznej migawki. Wady: Przy bardzo krótkich czasach naświetlania (np. powyżej 1/2000s) i przy użyciu obiektywów o bardzo dużym otworze przysłony, EFCS może czasami powodować nierównomierną ekspozycję lub zniekształcenie kształtu punktów świetlnych w nieostrości (bokeh). Zastosowanie: Funkcja EFCS jest dostępna w wielu nowoczesnych aparatach bezlusterkowych i niektórych lustrzankach. Każdy typ migawki posiada swoją specyfikę, która predestynuje go do określonych zastosowań i wpływa na możliwości aparatu oraz potencjalne artefakty na zdjęciach.\nAnomalie związane z pracą migawki # Niedoskonałości konstrukcyjne lub specyfika działania różnych typów migawek mogą prowadzić do pewnych niepożądanych efektów na zdjęciach.\nEfekt \u0026ldquo;rolling shutter\u0026rdquo; (migawka krocząca/szczelinowa elektroniczna): Przyczyna: Zjawisko to jest charakterystyczne przede wszystkim dla migawek elektronicznych, w których obraz nie jest odczytywany z całej powierzchni matrycy w jednej chwili, lecz sekwencyjnie – linia po linii pikseli (lub kolumna po kolumnie) z niewielkim, ale jednak istniejącym opóźnieniem między odczytem kolejnych linii. W pewnym, choć znacznie mniejszym stopniu, podobny efekt może wystąpić przy migawce mechanicznej szczelinowej, gdy fotografujemy bardzo szybko poruszające się obiekty przy czasach naświetlania wymagających ruchu szczeliny. Wizualne skutki: Najbardziej widoczne przy rejestracji szybkiego ruchu. Obiekty poruszające się szybko w poprzek kadru mogą ulec deformacji – np. pionowe obiekty (słupy, budynki) mogą wydawać się pochylone, a szybko obracające się elementy (śmigła helikoptera, struny gitary) mogą przybrać nienaturalny, \u0026ldquo;gumowy\u0026rdquo; kształt. Szybkie panoramowanie aparatem również może prowadzić do zniekształceń obrazu (tzw. efekt \u0026ldquo;jello\u0026rdquo;). Migawka globalna (Global Shutter): Jest to technologia odczytu matrycy, w której wszystkie piksele rozpoczynają i kończą naświetlanie dokładnie w tym samym momencie. Eliminuje to całkowicie efekt rolling shutter. Matryce z migawką globalną są jednak bardziej skomplikowane i droższe w produkcji, dlatego stosuje się je głównie w profesjonalnych kamerach wideo i niektórych specjalistycznych aparatach fotograficznych. Problemy z synchronizacją z lampą błyskową (czas X-sync): Wyjaśnienie: Jak już wspomniano przy omawianiu migawki szczelinowej, przy krótkich czasach naświetlania (krótszych niż czas X-sync, który dla większości aparatów wynosi od 1/125s do 1/250s), przez powierzchnię materiału światłoczułego przesuwa się jedynie wąska szczelina. Błysk lampy fotograficznej jest zjawiskiem bardzo krótkotrwałym (zazwyczaj ułamki milisekundy). Jeśli czas naświetlania ustawiony w aparacie jest krótszy niż czas potrzebny na pełne otwarcie się pierwszej kurtynki i odsłonięcie całej matrycy, to standardowy, pojedynczy błysk lampy oświetli tylko ten fragment matrycy, który był akurat odsłonięty przez przesuwającą się szczelinę w momencie wyzwolenia błysku. Skutek: Na zdjęciu pojawia się charakterystyczny ciemny pas (lub nawet cała część kadru jest czarna), odpowiadający fragmentowi matrycy, który w momencie błysku był jeszcze (lub już) zasłonięty przez jedną z kurtynek migawki. Tryb HSS (High-Speed Sync) / Auto FP (Focal Plane): Aby umożliwić stosowanie lampy błyskowej z bardzo krótkimi czasami naświetlania, opracowano specjalny tryb pracy lampy nazywany HSS (w systemie Canon i innych) lub Auto FP (w systemie Nikon). W tym trybie lampa błyskowa nie emituje jednego, krótkiego błysku, lecz serię bardzo szybkich, następujących po sobie krótkich błysków (efekt pulsowania) przez cały czas, gdy szczelina migawki przesuwa się nad powierzchnią matrycy. Zapewnia to równomierne oświetlenie całego kadru, nawet przy czasach rzędu 1/4000s czy 1/8000s. Główną wadą trybu HSS/Auto FP jest znaczący spadek efektywnej mocy (zasięgu) lampy błyskowej, ponieważ energia błysku jest rozłożona na dłuższy czas. Zrozumienie tych potencjalnych anomalii pozwala fotografowi unikać problemów technicznych, świadomie dobierać ustawienia aparatu i lampy błyskowej oraz w pełni wykorzystywać dostępne technologie. Wybór rodzaju migawki (lub trybu jej pracy w aparatach hybrydowych) jest często kompromisem między różnymi cechami, takimi jak szybkość, cichość pracy, brak drgań, a potencjalnymi artefaktami obrazu. Fotograf powinien być świadomy, jaki typ migawki posiada jego aparat i jakie są jego charakterystyki, aby móc wybrać optymalne ustawienia do danej sytuacji zdjęciowej.\nTabela: Porównanie Typów Migawek # Poniższa tabela syntetyzuje kluczowe cechy różnych typów migawek, ułatwiając zrozumienie ich specyfiki, zalet i wad. Jest to istotne dla świadomego wyboru sprzętu i technik fotografowania w zależności od potrzeb.\nCecha Migawka Szczelinowa (Kurtynowa) Migawka Centralna (Listkowa/Irysowa) Migawka Elektroniczna Migawka Hybrydowa (np. EFCS) Lokalizacja Przed materiałem światłoczułym (w korpusie) 15 W obiektywie, blisko przysłony 14 Zintegrowana z matrycą światłoczułą 14 Połączenie elementów elektronicznych i mechanicznych 14 Zasada działania Dwie kurtynki/lamelki tworzące szczelinę lub pełne otwarcie 16 Listki otwierające się od środka na zewnątrz 14 Elektroniczny odczyt pikseli matrycy 14 Np. pierwsza kurtyna elektroniczna, druga mechaniczna 17 Typowe czasy naświetlania Bardzo szeroki zakres, np. 30s do 1/8000s (lub krócej) 14 Ograniczony, np. 1s do 1/2000s 14 Bardzo szeroki, np. 30s do 1/32000s (lub krócej) 14 Zależne od implementacji Synchronizacja z lampą Ograniczona do X-sync (np. 1/250s), HSS dla krótszych czasów 19 Przy wszystkich czasach naświetlania Może być ograniczona, często wolniejsza niż mechaniczna 17 Zależne od implementacji, często jak mechaniczna druga kurtyna Głośność pracy Słyszalna (klik) Cicha 15 Bezdźwięczna 15 Cichsza niż w pełni mechaniczna Efekt \u0026ldquo;Rolling Shutter\u0026rdquo; Minimalny lub brak przy czasach X-sync, możliwy przy b. krótkich Brak Często występuje (jeśli nie ma global shutter) 17 Może wystąpić od elektronicznej części Główne Zalety Krótkie czasy naświetlania, stosowana w systemach wymiennej optyki Doskonała synchronizacja z lampą, cicha praca Bezdźwięczność, brak drgań, b. krótkie czasy, szybkie serie Redukcja drgań (shutter shock), cichsza praca Główne Wady Głośniejsza, ograniczona X-sync, drgania (shutter shock) Trudniejsze b. krótkie czasy, komplikuje konstrukcję obiektywu Rolling shutter, potencjalne problemy z migotaniem światła Potencjalne artefakty przy krótkich czasach i jasnych obiektywach Typowe Zastosowania Lustrzanki, bezlusterkowce 15 Aparaty średnioformatowe, niektóre kompakty, obiektywy wielkoformatowe 15 Bezlusterkowce, smartfony, kamery video 15 Wiele nowoczesnych bezlusterkowców "},{"id":4,"href":"/photography-basics/docs/klisza/","title":"Materiał światłoczuły","section":"Docs","content":" Materiał światłoczuły # Materiał światłoczuły jest kluczowym elementem aparatu fotograficznego, odpowiedzialnym za faktyczną rejestrację obrazu rzutowanego przez obiektyw. Przez dziesięciolecia dominowała fotografia analogowa oparta na kliszy, jednak rewolucja cyfrowa wprowadziła matryce elektroniczne, które obecnie są standardem. Zrozumienie zasad działania obu tych technologii pozwala docenić zarówno dziedzictwo fotografii chemicznej, jak i możliwości oferowane przez współczesne sensory cyfrowe.\nKlisza jako materiał światłoczuły - krótka historia, rodzaje i jak działa # Fotografia analogowa, oparta na chemicznym zapisie obrazu, ma bogatą historię i unikalne właściwości, które wciąż przyciągają entuzjastów.\nKrótka historia materiałów światłoczułych # Początki fotografii sięgają pierwszej połowy XIX wieku. Za pierwszą trwałą fotografię uznaje się dzieło Josepha Nicéphore\u0026rsquo;a Niépce\u0026rsquo;a z 1826 roku, wykonane techniką heliografii. Użył on płyty cynowej pokrytej światłoczułym bitumem (asfaltem syryjskim), a czas naświetlania wynosił wiele godzin. Przełomem była współpraca Niépce\u0026rsquo;a z Louisem Daguerre\u0026rsquo;em, która zaowocowała wynalezieniem dagerotypii w 1839 roku. Technika ta polegała na naświetlaniu posrebrzanej płytki miedzianej poddanej działaniu par jodu, a następnie wywoływaniu obrazu w oparach rtęci. Dagerotypia znacznie skróciła czas ekspozycji i oferowała lepszą ostrość obrazu. Dalsza ewolucja materiałów światłoczułych prowadziła od ciężkich i kruchych płyt metalowych i szklanych do wynalezienia elastycznego podłoża dla emulsji światłoczułej. Kluczową rolę odegrał tu George Eastman, założyciel firmy Kodak, który w 1888 roku wprowadził na rynek pierwszy aparat fotograficzny (Kodak No. 1) wykorzystujący zwijaną błonę fotograficzną, co znacznie spopularyzowało fotografię i uczyniło ją dostępną dla mas.\nBudowa kliszy fotograficznej # Typowa klisza fotograficzna, niezależnie od jej rodzaju, składa się z kilku podstawowych warstw:\nPodłoże: Jest to elastyczna, przezroczysta taśma, która stanowi nośnik dla pozostałych warstw. Początkowo używano azotanu celulozy, później bezpieczniejszego octanu celulozy, a współcześnie najczęściej stosuje się poliester ze względu na jego wytrzymałość i stabilność wymiarową. Emulsja światłoczuła: To najważniejsza, robocza warstwa kliszy. Jest to zawiesina mikroskopijnych kryształów halogenków srebra (najczęściej bromku srebra AgBr, ale także chlorku srebra AgCl i jodku srebra AgI) w żelatynie. Żelatyna pełni rolę nie tylko spoiwa dla kryształów, ale także aktywnie uczestniczy w procesie fotochemicznym, wpływając na czułość i inne właściwości emulsji. Wielkość i kształt kryształów halogenków srebra mają wpływ na czułość i ziarnistość filmu. Warstwy dodatkowe: Klisze mogą posiadać również inne warstwy, takie jak: Warstwa przeciwodblaskowa (antyhalacyjna): Umieszczona na spodzie podłoża lub między podłożem a emulsją, zapobiega odbiciu się światła od tylnej powierzchni podłoża i ponownemu naświetleniu emulsji, co mogłoby powodować powstawanie tzw. efektu halo wokół jasnych obiektów. Warstwa ochronna: Cienka warstwa żelatyny na powierzchni emulsji, chroniąca ją przed uszkodzeniami mechanicznymi (zarysowaniami). W przypadku filmów kolorowych struktura jest bardziej złożona i zawiera wiele warstw emulsji, każda uczulona na inny zakres widma światła, oraz warstwy filtrów barwnych. Zasada działania kliszy – proces fotochemiczny # Proces rejestracji obrazu na kliszy fotograficznej jest złożonym procesem fotochemicznym, który można podzielić na kilka kluczowych etapów:\nTworzenie obrazu utajonego (Latent Image Formation):\nGdy światło pada na emulsję światłoczułą, fotony oddziałują z kryształami halogenków srebra. Każdy kryształ składa się z regularnie ułożonych jonów srebra (Ag+) i jonów halogenu (np. bromu Br−). Energia fotonu może wybić elektron z jonu bromkowego. Ten uwolniony elektron może następnie zostać przechwycony przez jon srebra Ag+, redukując go do atomu metalicznego srebra (Ag). Reakcję tę można uprościć jako: AgBr+sˊwiatło→Ag+Br. Powstanie kilku takich atomów srebra w jednym krysztale tworzy tzw. zarodek obrazu utajonego lub centrum czułości. Są to mikroskopijne zmiany, niewidoczne gołym okiem, ale stanowią one miejsca, które będą preferencyjnie reagować podczas późniejszego procesu wywoływania. Ilość powstałych zarodków srebra w danym obszarze emulsji jest proporcjonalna do ilości światła (liczby fotonów), która na ten obszar padła. Proces wywoływania (Development):\nPo naświetleniu klisza jest zanurzana w roztworze chemicznym zwanym wywoływaczem. Wywoływacz zawiera substancje redukujące (np. metol, hydrochinon, fenidon), które mają zdolność przekształcania jonów srebra w metaliczne srebro. Kluczowe jest to, że proces redukcji zachodzi znacznie szybciej w tych kryształach halogenku srebra, które zawierają zarodki obrazu utajonego (czyli zostały naświetlone). W ten sposób niewidoczny obraz utajony jest wielokrotnie wzmacniany – naświetlone kryształy zamieniają się w ziarenka czarnego, metalicznego srebra. Obszary kliszy, które otrzymały więcej światła, stają się po wywołaniu ciemniejsze (gęstsze), natomiast obszary słabo naświetlone pozostają jaśniejsze. Proces przerywania (Stop Bath):\nAby precyzyjnie zakończyć działanie wywoływacza (który jest zazwyczaj zasadowy), kliszę przenosi się do kąpieli przerywającej. Jest to najczęściej słaby roztwór kwasu (np. kwasu octowego), który neutralizuje wywoływacz i zatrzymuje proces dalszej redukcji srebra. Proces utrwalania (Fixing):\nPo wywołaniu i przerwaniu, na kliszy wciąż znajdują się nienaświetlone i niewywołane kryształy halogenków srebra, które nadal są wrażliwe na światło. Aby uczynić obraz trwałym i niewrażliwym na dalsze działanie światła, kliszę poddaje się procesowi utrwalania. Polega on na zanurzeniu jej w roztworze utrwalacza (najczęściej jest to tiosiarczan sodu lub amonu). Utrwalacz rozpuszcza pozostałe, nienaświetlone halogenki srebra, przekształcając je w rozpuszczalne w wodzie związki kompleksowe, które są następnie wypłukiwane z emulsji. Bez tego etapu obraz na kliszy z czasem uległby całkowitemu zaczernieniu pod wpływem światła. Płukanie i suszenie:\nOstatnimi etapami są dokładne płukanie kliszy w czystej wodzie, aby usunąć wszelkie pozostałości chemikaliów, a następnie ostrożne suszenie w bezpyłowym środowisku. W wyniku całego tego procesu na kliszy powstaje obraz negatywowy. Oznacza to, że jasności i (w przypadku filmów kolorowych) kolory są odwrócone w stosunku do oryginalnej sceny: obszary, które były jasne w rzeczywistości, na negatywie są ciemne (gęste od ziaren srebra), a obszary ciemne są na negatywie jasne (przezroczyste). Aby uzyskać obraz pozytywowy, czyli zdjęcie w formie, jaką znamy, negatyw musi zostać skopiowany na inny materiał światłoczuły, np. papier fotograficzny (który działa na podobnej zasadzie chemicznej) lub zeskanowany cyfrowo i odwrócony w programie graficznym.\nRodzaje klisz fotograficznych # Klisze fotograficzne można podzielić na kilka podstawowych rodzajów, różniących się sposobem rejestracji obrazu i przeznaczeniem:\nKlisze czarno-białe (Monochromatic Films): Rejestrują jedynie informacje o luminancji (jasności) poszczególnych fragmentów sceny, tworząc obraz składający się z odcieni szarości, od głębokiej czerni do czystej bieli. Emulsja takich filmów zawiera zazwyczaj jeden rodzaj halogenków srebra. Są cenione za klasyczną estetykę, możliwość pełnej kontroli nad procesem wywoływania (co pozwala wpływać na kontrast i ziarnistość) oraz archiwalną trwałość. Przykłady popularnych filmów czarno-białych to Ilford HP5 Plus, Kodak Tri-X 400, Fomapan 100/200/400. Klisze kolorowe negatywowe (Color Negative Films): Są to najpopularniejsze filmy kolorowe. Po wywołaniu tworzą negatyw, na którym nie tylko jasności, ale również kolory są odwrócone (tzn. barwy są komplementarne do oryginalnych – np. czerwony staje się cyjanowy, zielony magentowy, a niebieski żółty). Emulsja filmów kolorowych negatywowych jest znacznie bardziej złożona niż w filmach czarno-białych. Składa się zazwyczaj z trzech głównych warstw światłoczułych, z których każda jest uczulona na inny zakres widma światła widzialnego (jedna na światło niebieskie, druga na zielone, a trzecia na czerwone). Każda z tych warstw zawiera odpowiednie sprzęgacze barwnikowe, które podczas specjalnego procesu wywoływania (najczęściej C-41) reagują z produktami utleniania wywoływacza barwotwórczego, tworząc barwniki (żółty, magentowy i cyjanowy) w ilościach proporcjonalnych do naświetlenia danej warstwy. Filmy te charakteryzują się dużą tolerancją na błędy naświetlania i elastycznością podczas wykonywania odbitek lub skanowania. Przykłady to Kodak Gold 200, Kodak Portra 400, Fujifilm Superia X-TRA 400. Klisze kolorowe pozytywowe (slajdy/diapozytywy, Color Reversal/Slide Films): Ten typ filmów, w przeciwieństwie do negatywów, po wywołaniu daje bezpośrednio obraz pozytywowy, czyli z naturalnymi jasnościami i kolorami, gotowy do oglądania (np. po oprawieniu w ramki jako slajdy i rzutowaniu na ekran) lub skanowania. Proces wywoływania filmów pozytywowych (najczęściej E-6) jest jeszcze bardziej skomplikowany niż C-41 i obejmuje m.in. pierwsze wywołanie czarno-białe, a następnie specjalne etapy chemicznego lub świetlnego zadymiania i drugiego wywoływania barwotwórczego, które prowadzi do \u0026ldquo;odwrócenia\u0026rdquo; obrazu. Slajdy są cenione za bardzo żywe, nasycone kolory, wysoki kontrast i dużą ostrość. Są jednak znacznie mniej tolerancyjne na błędy naświetlania niż filmy negatywowe – wymagają bardzo precyzyjnej ekspozycji. Przykłady to Fujichrome Velvia 50 (znana z intensywnych barw, idealna do krajobrazów) czy Kodak Ektachrome E100. Podstawowe właściwości klisz # Niezależnie od rodzaju, klisze fotograficzne charakteryzują się kilkoma podstawowymi właściwościami, które determinują ich zachowanie i finalny wygląd obrazu:\nCzułość (ISO/ASA/DIN): Jest to miara wrażliwości emulsji na światło. Im wyższa wartość ISO (International Organization for Standardization) lub ASA (American Standards Association – starszy system, ale wartości liczbowe są te same co ISO), tym film jest bardziej czuły na światło, co oznacza, że wymaga krótszego czasu naświetlania lub mniejszego otworu przysłony do uzyskania prawidłowej ekspozycji. Filmy o niskiej czułości (np. ISO 50, 100, 200) wymagają dużo światła (lub długich czasów ekspozycji/dużych otworów przysłony) i zazwyczaj dają obraz o bardzo drobnym ziarnie i wysokiej rozdzielczości. Filmy o wysokiej czułości (np. ISO 800, 1600, 3200, a nawet wyższe) pozwalają na fotografowanie w słabych warunkach oświetleniowych lub z krótkimi czasami naświetlania, ale zazwyczaj kosztem większej ziarnistości i potencjalnie niższego kontrastu oraz mniejszej rozdzielczości. Podwojenie wartości ISO oznacza dwukrotny wzrost czułości, co pozwala skrócić czas naświetlania o połowę lub zmniejszyć otwór przysłony o jedną pełną działkę (1 EV) przy zachowaniu tej samej ekspozycji. Skala DIN (Deutsche Industrie Norm) jest logarytmiczna – wzrost o 3° DIN odpowiada podwojeniu czułości ASA/ISO. Ziarnistość (Grain): Jest to widoczna, losowa tekstura obrazu fotograficznego, wynikająca z nieregularnego rozkładu i wielkości ziaren metalicznego srebra (w filmach czarno-białych) lub chmurek barwnika (w filmach kolorowych) tworzących obraz w emulsji. Ziarnistość jest zazwyczaj bardziej widoczna w filmach o wyższej czułości, przy dużych powiększeniach oraz w niektórych specyficznych typach filmów lub przy użyciu określonych wywoływaczy. Dla jednych fotografów ziarno jest niepożądanym artefaktem obniżającym jakość obrazu, dla innych – świadomym środkiem artystycznego wyrazu, dodającym zdjęciom charakteru. Format: Określa fizyczne wymiary kliszy, a co za tym idzie – rozmiar pojedynczej klatki obrazu. Najpopularniejsze formaty to: 35mm (małoobrazkowy, typ 135): Najbardziej rozpowszechniony format, klatka o wymiarach 24x36 mm. Oferuje dobry kompromis między jakością obrazu a poręcznością sprzętu. Średni format (Medium Format, np. typ 120, 220): Klisza w rolce, szersza niż 35mm, pozwala na uzyskanie znacznie większych klatek (np. 6x4.5 cm, 6x6 cm, 6x7 cm, 6x9 cm). Większy format negatywu/pozytywu przekłada się na lepszą jakość obrazu – mniejszą ziarnistość przy tym samym powiększeniu, większą szczegółowość i lepszą tonalność. Wielki format (Large Format): Wykorzystuje pojedyncze arkusze filmu (np. 4x5 cala, 8x10 cala), oferując najwyższą możliwą jakość obrazu, ale wymagając specjalistycznych, dużych i mniej poręcznych aparatów. Proces fotografii analogowej jest złożonym procesem chemicznym, gdzie precyzja naświetlania i kontrolowane warunki wywoływania są kluczowe dla uzyskania pożądanego obrazu. Obraz utajony jest niezwykle delikatny i niewidoczny 22, a proces wywoływania wzmacnia go miliony razy, wymagając precyzji pod względem czasu, temperatury i składu chemicznego. Błędy na etapie naświetlania są trudniejsze do skorygowania niż w fotografii cyfrowej, a proces utrwalania jest niezbędny dla trwałości obrazu. Wybór rodzaju kliszy, jej czułości i ziarnistości jest fundamentalną decyzją artystyczną, silnie wpływającą na estetykę zdjęcia jeszcze przed jego wykonaniem. Różne typy klisz mają różne charakterystyki tonalne, kontrast, nasycenie barw i strukturę ziarna. W fotografii analogowej \u0026ldquo;wygląd\u0026rdquo; zdjęcia jest w dużej mierze zdeterminowany już na etapie wyboru i naświetlania materiału.\n"},{"id":5,"href":"/photography-basics/docs/matryca/","title":"Matryca jako materiał światłoczuły","section":"Docs","content":" Matryca jako materiał światłoczuły # Wprowadzenie do matrycy: serce aparatu cyfrowego # Matryca światłoczuła jest fundamentalnym komponentem każdego aparatu cyfrowego, pełniącym rolę analogiczną do błony filmowej w fotografii tradycyjnej. Jej zadaniem jest przechwytywanie światła wpadającego przez obiektyw i przekształcanie go na sygnał elektryczny.1 Ten sygnał jest następnie przetwarzany przez procesor obrazu w celu stworzenia finalnego zdjęcia cyfrowego.1 Zrozumienie zasady działania matrycy jest kluczowe dla pojęcia całego procesu fotograficznego w erze cyfrowej.\nKażda matryca zbudowana jest z milionów miniaturowych elementów światłoczułych, nazywanych pikselami.1 Każdy z tych pikseli działa jak mały detektor, rejestrujący intensywność światła, które na niego pada w danym momencie ekspozycji. To właśnie z informacji zebranych przez poszczególne piksele odtwarzany jest cały obraz.4\nFizyka działania fotodiody w pikselu # Sercem każdego piksela, zarówno w matrycach typu CCD, jak i CMOS, jest fotodioda. Jest to element półprzewodnikowy, którego podstawową funkcją jest konwersja energii świetlnej (fotonów) na ładunek elektryczny (elektrony).\nStruktura P-N i polaryzacja wsteczna # Fotodiody są zazwyczaj strukturami półprzewodnikowymi typu P-N, co oznacza, że składają się z dwóch warstw materiału półprzewodnikowego (najczęściej krzemu) o różnym typie domieszkowania: typu P (z nadmiarem dziur, czyli dodatnich nośników ładunku) i typu N (z nadmiarem elektronów, czyli ujemnych nośników ładunku).6 Na granicy tych dwóch warstw tworzy się tzw. złącze P-N, a wokół niego powstaje obszar zubożony (ang. depletion region), pozbawiony swobodnych nośników ładunku.8\nW matrycach światłoczułych fotodiody pracują zazwyczaj w trybie polaryzacji wstecznej (ang. reverse bias).6 Oznacza to, że do warstwy typu P przykładany jest potencjał ujemny, a do warstwy typu N potencjał dodatni. Taka polaryzacja prowadzi do poszerzenia obszaru zubożonego, co ma kluczowe znaczenie dla efektywności detekcji światła. Szerszy obszar zubożony zwiększa prawdopodobieństwo, że padający foton zostanie zaabsorbowany właśnie w tym obszarze, gdzie wygenerowane nośniki ładunku mogą być efektywnie rozdzielone i zebrane. Ponadto, polaryzacja wsteczna zmniejsza pojemność złącza, co przekłada się na szybszą odpowiedź fotodiody na zmiany oświetlenia.8\nProces konwersji fotonów na elektrony (generowanie par elektron-dziura) # Gdy foton o energii równej lub większej niż szerokość przerwy energetycznej materiału półprzewodnikowego (dla krzemu jest to około 1. eV, co odpowiada światłu widzialnemu i bliskiej podczerwieni) pada na fotodiodę, może zostać zaabsorbowany.9 Absorpcja fotonu skutkuje wybiciem elektronu z pasma walencyjnego do pasma przewodnictwa. W miejscu, z którego elektron został wybity, powstaje \u0026ldquo;dziura\u0026rdquo; o ładunku dodatnim. W ten sposób tworzona jest para elektron-dziura.6 Ten proces, znany jako wewnętrzny efekt fotoelektryczny, jest podstawą działania fotodiody. Istotne jest, że liczba wygenerowanych par elektron-dziura jest wprost proporcjonalna do liczby padających fotonów, a więc do natężenia światła.\nGromadzenie ładunku w studni potencjału, separacja nośników przez pole elektryczne # W obszarze zubożonym, poszerzonym przez polaryzację wsteczną, istnieje silne wewnętrzne pole elektryczne. To pole odgrywa kluczową rolę w separacji nowo powstałych par elektron-dziura.8 Elektrony (o ładunku ujemnym) są przyciągane w kierunku dodatnio naładowanej warstwy N, natomiast dziury (o ładunku dodatnim) przemieszczają się w kierunku ujemnie naładowanej warstwy P.9\nElektrony, które są nośnikami sygnału w większości matryc, są następnie gromadzone w specjalnie ukształtowanym obszarze pod elektrodą piksela, zwanym \u0026ldquo;studnią potencjału\u0026rdquo; (ang. potential well).4 Pojemność tej studni, określana jako Full Well Capacity (FWC), definiuje maksymalną liczbę elektronów, jaką dany piksel może zgromadzić przed osiągnięciem stanu nasycenia. Im więcej fotonów padnie na piksel, tym więcej elektronów zgromadzi się w studni potencjału, aż do jej całkowitego wypełnienia.\nPrąd ciemny i jego wpływ # Nawet w całkowitej ciemności, bez padającego światła, w materiale półprzewodnikowym zachodzą procesy termicznego generowania par elektron-dziura. Prowadzi to do powstawania niewielkiego prądu, zwanego prądem ciemnym (ang. dark current).7 Prąd ciemny jest niepożądanym zjawiskiem, ponieważ dodaje szum do sygnału użytecznego (pochodzącego od światła) i może ograniczać zdolność detekcji bardzo słabych sygnałów.8 Poziom prądu ciemnego jest silnie zależny od temperatury matrycy – rośnie wykładniczo wraz ze wzrostem temperatury – oraz od czasu ekspozycji. Z tego powodu w zaawansowanych zastosowaniach, np. w astrofotografii, matryce są często chłodzone, aby zminimalizować prąd ciemny i poprawić stosunek sygnału do szumu.3\nRodzaje matryc światłoczułych # Na rynku dominują dwa główne typy matryc światłoczułych: CCD (Charge-Coupled Device) oraz CMOS (Complementary Metal-Oxide-Semiconductor). Chociaż podstawowy mechanizm konwersji światła na ładunek w pikselu jest w obu przypadkach podobny (oparty na fotodiodzie), różnią się one znacząco architekturą, sposobem odczytu sygnału oraz charakterystyką działania.\nCCD (Charge-Coupled Device) # Matryce CCD były historycznie pierwszym szeroko stosowanym typem sensorów w aparatach cyfrowych i przez długi czas uznawane były za oferujące wyższą jakość obrazu.\nArchitektura piksela i zasada działania: W matrycy CCD ładunek (elektrony) zgromadzony w każdym pikselu podczas ekspozycji jest transferowany sekwencyjnie, niczym woda w wiadrach przekazywanych z rąk do rąk. Ładunki z poszczególnych pikseli w rzędzie są przesuwane do rejestru odczytu, a następnie ładunki z tego rejestru są kolejno transferowane do jednego, wspólnego dla całej matrycy (lub jej części) wzmacniacza i przetwornika analogowo-cyfrowego (ADC).2 Ten proces powtarzany jest dla każdego rzędu pikseli.2 Zalety: Matryce CCD charakteryzowały się tradycyjnie bardzo dobrą jakością obrazu, niskim poziomem szumów (szczególnie szumu stałowzorcowego, dzięki scentralizowanemu wzmacnianiu), wysoką czułością oraz dużym współczynnikiem wypełnienia piksela (ang. fill factor), co oznacza, że większa część powierzchni piksela była aktywna światłoczułe. Naturalną cechą matryc CCD jest również tzw. migawka globalna (ang. global shutter), gdzie wszystkie piksele rozpoczynają i kończą ekspozycję jednocześnie, co jest korzystne przy fotografowaniu szybko poruszających się obiektów. Wady: Głównymi wadami matryc CCD są: stosunkowo wolny odczyt danych (konieczność sekwencyjnego transferu ładunków), wyższe zużycie energii w porównaniu do CMOS oraz bardziej skomplikowany i droższy proces produkcji. Są również podatne na zjawisko zwane \u0026ldquo;bloomingiem\u0026rdquo;, czyli przelewaniem się nadmiaru ładunku z nasyconego piksela do sąsiednich, co objawia się jako pionowe smugi na obrazie w miejscu silnych prześwietleń. CMOS (Complementary Metal-Oxide-Semiconductor) # Matryce CMOS, początkowo stosowane w mniej wymagających aplikacjach, dzięki intensywnemu rozwojowi technologicznemu zdominowały rynek aparatów cyfrowych, od smartfonów po profesjonalne aparaty pełnoklatkowe.2\nArchitektura piksela (Active Pixel Sensor - APS): Kluczową różnicą w matrycach CMOS jest to, że każdy piksel (lub mała grupa pikseli) posiada własną, zintegrowaną elektronikę, w tym wzmacniacz sygnału, a często także elementy przetwornika analogowo-cyfrowego. Taka architektura nazywana jest Aktywnym Pikselem Sensorycznym (APS). Ładunek zgromadzony w fotodiodzie jest konwertowany na napięcie bezpośrednio w obrębie piksela, a następnie to napięcie jest odczytywane indywidualnie dla każdego piksela poprzez adresowanie, podobnie jak w pamięciach komputerowych.2 Zalety: Matryce CMOS oferują znacznie szybszy odczyt danych niż CCD, co umożliwia realizację takich funkcji jak szybkie zdjęcia seryjne, nagrywanie wideo w wysokiej rozdzielczości i z dużą liczbą klatek na sekundę, a także selektywny odczyt fragmentów obrazu (\u0026ldquo;okienkowanie\u0026rdquo;). Charakteryzują się również znacznie niższym zużyciem energii oraz niższym kosztem produkcji, ponieważ mogą być wytwarzane w standardowych procesach technologicznych używanych do produkcji innych układów scalonych CMOS (np. procesorów, pamięci). Pozwalają także na większą integrację dodatkowych funkcji (np. przetwarzania obrazu, redukcji szumów) bezpośrednio na chipie matrycy.2 Wady: Wczesne generacje matryc CMOS cierpiały na wyższy poziom szumów w porównaniu do CCD, szczególnie szumu stałowzorcowego (FPN), wynikającego z niejednorodności wzmacniaczy w poszczególnych pikselach.2 Miały również niższą czułość i mniejszy współczynnik wypełnienia piksela, ponieważ część jego powierzchni zajmowała elektronika. Większość matryc CMOS wykorzystuje tzw. migawkę kroczącą (ang. rolling shutter), gdzie kolejne linie pikseli są odczytywane sekwencyjnie, co może prowadzić do zniekształceń obrazu szybko poruszających się obiektów (efekt \u0026ldquo;galaretki\u0026rdquo;).2 Dostępne są jednak również matryce CMOS z migawką globalną, eliminujące ten problem. Nowoczesne technologie CMOS: BSI (Back-Side Illumination): Technologia \u0026ldquo;oświetlenia od tyłu\u0026rdquo; polega na odwróceniu tradycyjnej struktury piksela CMOS. Warstwa z połączeniami metalicznymi i tranzystorami jest przenoszona pod warstwę światłoczułą (krzemową). Dzięki temu światło padające na matrycę ma bezpośredni dostęp do fotodiod, nie będąc blokowane ani rozpraszane przez elementy elektroniki. Skutkuje to znacznym wzrostem współczynnika wypełnienia i efektywności zbierania światła, co przekłada się na wyższą czułość i lepszą jakość obrazu, szczególnie w przypadku pikseli o małych rozmiarach.2 Stacked CMOS (Matryce warstwowe): W tej zaawansowanej technologii warstwa pikseli jest oddzielona od warstwy obwodów przetwarzających sygnał i ułożona nad nią. Obie warstwy są ze sobą połączone. Pozwala to na jeszcze większą integrację zaawansowanych funkcji przetwarzania obrazu bezpośrednio na chipie matrycy oraz na bardzo szybki odczyt danych, co jest kluczowe np. dla wideo w ultra wysokiej rozdzielczości i z ekstremalnie dużą liczbą klatek na sekundę. Ewolucja technologii CMOS, napędzana potrzebą miniaturyzacji, szybkości i niższych kosztów, doprowadziła do przezwyciężenia wielu początkowych wad. Presja rynkowa faworyzowała rozwój CMOS, a technologie takie jak BSI stanowiły przełom, rozwiązując problem niskiego współczynnika wypełnienia i czułości. W rezultacie, nowoczesne matryce CMOS oferują jakość obrazu porównywalną, a w niektórych aspektach nawet przewyższającą matryce CCD, przy jednoczesnym zachowaniu swoich inherentnych zalet, takich jak szybkość i niskie zużycie energii. Zrozumienie tej ewolucji jest kluczowe dla docenienia obecnego stanu technologii sensorów i kierunków jej dalszego rozwoju.\nPoniższa tabela podsumowuje kluczowe cechy matryc CCD i CMOS:\nCecha CCD (Tradycyjna) CMOS (Wczesna) CMOS (Nowoczesna BSI/Stacked) Jakość obrazu (ogólnie) Bardzo dobra, niska szumowość Zwykle niższa, wyższe szumy FPN Bardzo dobra, porównywalna lub lepsza od CCD Czułość Wysoka Niższa Wysoka, szczególnie BSI Współczynnik wypełnienia Wysoki Niższy (elektronika w pikselu) Wysoki (dzięki BSI) Szumy (ogólnie) Niskie, zwłaszcza FPN Wyższe, zwłaszcza FPN Niskie, zaawansowana redukcja FPN Szybkość odczytu Wolna (sekwencyjny transfer) Szybsza (adresowalny odczyt) Bardzo szybka (równoległy odczyt, stacked) Zużycie energii Wysokie Niskie Bardzo niskie Koszt produkcji Wyższy (specjalistyczny proces) Niższy (standardowy proces CMOS) Niższy/umiarkowany Migawka (typowo) Globalna Krocząca (Rolling shutter) Krocząca lub Globalna Integracja funkcji Ograniczona Możliwa (np. ADC na chipie) Wysoka (zaawansowane przetwarzanie na chipie) Blooming Podatna Mniej podatna Dobrze kontrolowany Charakterystyka popularnych formatów matryc # Rozmiar fizyczny matrycy światłoczułej ma fundamentalny wpływ na wiele aspektów jakości obrazu i charakterystyki aparatu. Do najpopularniejszych formatów należą [12 (link zewn. fotoblogia.pl)]:\nPełna klatka (Full Frame, ok. 36x24 mm): Ten format odpowiada rozmiarowi pojedynczej klatki tradycyjnego filmu małoobrazkowego (35 mm). Matryce pełnoklatkowe, dzięki swojej dużej powierzchni, oferują zazwyczaj najlepszą jakość obrazu, charakteryzującą się niskim poziomem szumów (szczególnie przy wysokich czułościach ISO), szerokim zakresem dynamicznym oraz możliwością łatwego uzyskania małej głębi ostrości, co jest pożądane np. w fotografii portretowej. Są one standardem w profesjonalnych i zaawansowanych amatorskich aparatach cyfrowych (lustrzankach i bezlusterkowcach). APS-C (Advanced Photo System type-C): Matryce APS-C są mniejsze od pełnoklatkowych, a ich dokładne wymiary mogą się nieznacznie różnić w zależności od producenta (np. ok. 23.6x15.6 mm dla Nikon/Sony/Fuji lub 22.2x14.8 mm dla Canon). Stanowią one popularny wybór w lustrzankach i bezlusterkowcach skierowanych do entuzjastów fotografii, oferując dobry kompromis między jakością obrazu, rozmiarem aparatu i obiektywów oraz ceną systemu. Mikro Cztery Trzecie (Micro Four Thirds - MFT, ok. 17.3x13 mm): Ten format jest jeszcze mniejszy od APS-C i został opracowany wspólnie przez firmy Olympus (obecnie OM System) i Panasonic dla ich systemów aparatów bezlusterkowych. Mniejszy rozmiar matrycy MFT pozwala na konstruowanie bardziej kompaktowych aparatów i obiektywów, co jest zaletą dla osób ceniących mobilność. Wpływ rozmiaru matrycy na kąt widzenia (współczynnik kadrowania - crop factor):\nMniejsze matryce, w porównaniu do matrycy pełnoklatkowej, \u0026ldquo;widzą\u0026rdquo; węższy fragment obrazu rzutowanego przez ten sam obiektyw. Zjawisko to opisuje tzw. współczynnik kadrowania (ang. crop factor). Jest to liczba, przez którą należy pomnożyć nominalną ogniskową obiektywu, aby uzyskać ogniskową dającą taki sam kąt widzenia na matrycy pełnoklatkowej. Na przykład, dla matryc APS-C współczynnik kadrowania wynosi typowo ok. 1.5x (Nikon, Sony, Fuji) lub 1.6x (Canon), a dla systemu Mikro Cztery Trzecie ok. 2x. Oznacza to, że obiektyw o ogniskowej 50 mm założony na aparat z matrycą APS-C (crop 1.5x) da kąt widzenia odpowiadający obiektywowi 75 mm na pełnej klatce.\nWpływ rozmiaru matrycy na głębię ostrości:\nRozmiar matrycy ma również istotny wpływ na głębię ostrości. Przy tej samej wartości liczbowej przysłony (np. f/2.8) i tym samym kącie widzenia (co oznacza użycie obiektywu o krótszej rzeczywistej ogniskowej na mniejszej matrycy, aby uzyskać ten sam kadr co na matrycy pełnoklatkowej), mniejsze matryce dają większą głębię ostrości. Jest to złożone zjawisko, wynikające z faktu, że dla uzyskania tego samego kadru na mniejszej matrycy używamy obiektywu o krótszej ogniskowej, co samo w sobie prowadzi do większej głębi ostrości. Dodatkowo, aby uzyskać obraz o tej samej wielkości z mniejszej matrycy, wymagane jest większe powiększenie, co wpływa na postrzegany rozmiar krążka rozproszenia i tym samym na akceptowalną głębię ostrości. Zrozumienie tej zależności jest kluczowe, gdy celem jest świadome manipulowanie głębią ostrości, np. w celu uzyskania silnego rozmycia tła.\nPoniższa tabela zestawia charakterystykę popularnych formatów matryc:\nCecha Pełna klatka (Full Frame) APS-C Mikro Cztery Trzecie (MFT) Przybliżone wymiary (mm) 36 x 24 np. 23.6 x 15.6 lub 22.2 x 14.8 17.3 x 13 Współczynnik kadrowania 1x (referencyjny) ok. 1.5x - 1.6x ok. 2x Typowe zalety Najlepsza jakość obrazu, niskie szumy, duży DR, mała głębia ostrości (łatwość uzyskania) Dobry kompromis: jakość/rozmiar/cena, duży wybór systemów Kompaktowy rozmiar systemu, dobra jakość obrazu Typowe wady Duży rozmiar i waga systemu, wysoka cena Większa głębia ostrości niż FF (przy tym samym kadrze i f-stop), nieco wyższe szumy niż FF Mniejsza matryca = potencjalnie wyższe szumy i mniejszy DR niż większe formaty, większa głębia ostrości Przykładowe zastosowania Fotografia profesjonalna, portretowa, krajobrazowa, sportowa, studyjna Fotografia zaawansowana, hobbystyczna, podróżnicza, codzienna Fotografia podróżnicza, uliczna, filmowanie, systemy wymagające kompaktowości Wpływ wielkości piksela i gęstości pikseli na jakość obrazu # Parametry takie jak fizyczny rozmiar pojedynczego piksela oraz całkowita liczba pikseli na matrycy (gęstość pikseli) mają bezpośredni i złożony wpływ na kluczowe aspekty jakości obrazu, w tym czułość, poziom szumów, zakres dynamiczny oraz postrzeganą rozdzielczość.\nCzułość (ISO) i współczynnik wypełnienia (fill factor) # Większe piksele, ze względu na swoją większą powierzchnię, są w stanie zebrać więcej fotonów w danym czasie ekspozycji. Prowadzi to do silniejszego sygnału elektrycznego, co generalnie przekłada się na lepszy stosunek sygnału do szumu (SNR) i wyższą rzeczywistą czułość matrycy. Oznacza to, że przy słabym oświetleniu matryce z większymi pikselami mogą produkować obrazy o lepszej jakości z mniejszą ilością szumów przy tej samej wartości ISO.\nWspółczynnik wypełnienia (ang. fill factor) to parametr określający, jaka część całkowitej powierzchni piksela jest faktycznie światłoczuła. Im wyższy współczynnik wypełnienia, tym efektywniej piksel zbiera światło. W tradycyjnych matrycach CMOS (szczególnie typu FSI - Front-Side Illumination), część powierzchni piksela zajmowana jest przez tranzystory i metalowe ścieżki, co zmniejsza współczynnik wypełnienia.4 Problem ten jest częściowo rozwiązywany przez stosowanie mikrosoczewek nad każdym pikselem, które skupiają światło na obszarze światłoczułym.2 Jeszcze lepsze rezultaty daje wspomniana wcześniej technologia BSI, gdzie warstwa światłoczuła znajduje się nad elektroniką, maksymalizując efektywną powierzchnię zbierającą światło.2\nPoziom szumów # Szum jest nieodłącznym elementem obrazu cyfrowego i pochodzi z różnych źródeł. Jego poziom i charakter zależą od wielu czynników, w tym od wielkości pikseli.\nSzum śrutowy (Photon Shot Noise): Ten rodzaj szumu wynika bezpośrednio z kwantowej natury światła. Liczba fotonów docierających do danego piksela w określonym czasie podlega naturalnym fluktuacjom statystycznym (opisywanym przez rozkład Poissona). Szum śrutowy jest proporcjonalny do pierwiastka kwadratowego z natężenia sygnału (liczby zebranych fotonów) i jest najbardziej widoczny w jaśniejszych partiach obrazu. Większe piksele, zbierając więcej fotonów, generują silniejszy sygnał, co może prowadzić do lepszego stosunku sygnału do szumu śrutowego. Szum termiczny (Dark Current Noise): Jak wspomniano wcześniej, jest on wynikiem termicznego generowania elektronów w krzemie, nawet przy braku światła. Jego poziom rośnie wraz z temperaturą matrycy i czasem naświetlania. Chociaż większe piksele mogą mieć mniejszy prąd ciemny na jednostkę powierzchni, całkowity prąd ciemny zależy również od technologii wykonania i temperatury pracy. Szum odczytu (Read Noise): Jest to szum wprowadzany przez elektronikę matrycy podczas procesu odczytu ładunku zgromadzonego w pikselach i jego konwersji na sygnał cyfrowy. Obejmuje on szum wzmacniaczy, szum kwantyzacji przetwornika A/C oraz, w przypadku matryc CMOS, tzw. szum resetu (kTC noise). Szum odczytu jest generalnie niezależny od ilości światła (sygnału) i dominuje w najciemniejszych partiach obrazu. Wielkość piksela sama w sobie nie musi bezpośrednio determinować poziomu szumu odczytu, który bardziej zależy od architektury i jakości elektroniki odczytującej. Szum stałowzorcowy (Fixed Pattern Noise - FPN): Wynika z niewielkich, ale stałych różnic w charakterystyce poszczególnych pikseli, takich jak różnice w prądzie ciemnym (DSNU - Dark Signal Non-Uniformity) lub różnice w odpowiedzi na światło (PRNU - Photo Response Non-Uniformity). FPN był większym problemem we wczesnych matrycach CMOS, ale nowoczesne techniki kalibracji i korekcji wbudowane w aparaty i oprogramowanie znacznie go redukują. Większe piksele, dzięki zdolności do zbierania większej ilości fotonów (silniejszy sygnał), często oferują lepszy ogólny stosunek sygnału do szumu, co przekłada się na czystsze obrazy, szczególnie w trudnych warunkach oświetleniowych i przy wyższych czułościach ISO.\nZakres dynamiczny (Dynamic Range - DR) # Zakres dynamiczny matrycy określa jej zdolność do jednoczesnego rejestrowania bardzo jasnych i bardzo ciemnych obszarów sceny bez utraty szczegółów (tj. bez prześwietleń w światłach i bez utonięcia w szumach w cieniach). Jest on definiowany jako stosunek maksymalnego sygnału, jaki piksel może zarejestrować przed nasyceniem (określonego przez jego pojemność studni potencjału - FWC), do poziomu szumów w najciemniejszych obszarach (zazwyczaj zdominowanego przez szum odczytu).\nWiększe piksele zazwyczaj charakteryzują się większą pojemnością studni potencjału (FWC), ponieważ mają fizycznie więcej miejsca na gromadzenie elektronów. Przy podobnym poziomie szumu odczytu, większa FWC bezpośrednio przekłada się na szerszy zakres dynamiczny. Dlatego matryce z większymi pikselami (np. pełnoklatkowe o umiarkowanej liczbie megapikseli) często oferują lepszy zakres dynamiczny niż matryce z bardzo małymi, gęsto upakowanymi pikselami.\nPojemność studni potencjału (Full Well Capacity - FWC) # FWC to maksymalna liczba elektronów, jaką pojedynczy piksel może zgromadzić i przechować przed osiągnięciem stanu nasycenia. Po przekroczeniu FWC, piksel nie jest w stanie zarejestrować więcej światła, a nadmiarowy ładunek może \u0026ldquo;przelewać się\u0026rdquo; do sąsiednich pikseli (zjawisko bloomingu, bardziej typowe dla CCD) lub po prostu prowadzić do utraty informacji w prześwietlonych obszarach. FWC jest bezpośrednio związana z fizycznym rozmiarem fotodiody w pikselu – większa fotodioda generalnie oznacza większą FWC. Większa FWC jest pożądana, ponieważ przyczynia się do wyższego stosunku sygnału do szumu w jasnych partiach obrazu i szerszego zakresu dynamicznego.\nRozdzielczość a percepcja szczegółów # Większa liczba pikseli na matrycy (czyli wyższa gęstość pikseli) teoretycznie oznacza wyższą rozdzielczość, czyli zdolność do odwzorowania drobniejszych detali sceny. Jednakże, jeśli duża liczba pikseli jest upakowana na małej matrycy, poszczególne piksele stają się bardzo małe. Jak omówiono powyżej, bardzo małe piksele mogą cierpieć z powodu wyższego poziomu szumów, mniejszej czułości i węższego zakresu dynamicznego. W rezultacie, chociaż teoretyczna rozdzielczość może być wysoka, rzeczywista zdolność do rozróżniania drobnych szczegółów, zwłaszcza w trudnych warunkach oświetleniowych lub przy dużym kontraście sceny, może być ograniczona przez szumy i inne niedoskonałości sygnału. Istnieje zatem pewien kompromis między dążeniem do jak największej liczby megapikseli a utrzymaniem wysokiej \u0026ldquo;jakości\u0026rdquo; sygnału z każdego piksela. To wyjaśnia, dlaczego profesjonalne aparaty często stawiają na optymalny balans między rozdzielczością a wielkością piksela, zamiast na samą maksymalizację liczby megapikseli.\nZwiązek między fizycznym rozmiarem piksela a \u0026ldquo;jakością\u0026rdquo; sygnału jest fundamentalny. Większe piksele generalnie oferują lepszy stosunek sygnału do szumu (SNR), większy zakres dynamiczny (DR) i wyższą czułość. Wynika to z faktu, że większy piksel ma większą powierzchnię zbierającą fotony, co prowadzi do silniejszego sygnału dla danego poziomu oświetlenia. Ponadto, większy piksel często oznacza większą pojemność studni potencjału (FWC), co pozwala na zgromadzenie większej liczby elektronów przed nasyceniem. Przy założeniu, że poziom szumu odczytu jest w dużej mierze niezależny od rozmiaru piksela, a szum śrutowy jest zależny od sygnału, silniejszy sygnał i większa FWC prowadzą do lepszego SNR i DR. Dlatego, przy tej samej generacji technologii, matryce z większymi pikselami (np. pełnoklatkowe o umiarkowanej liczbie megapikseli) często lepiej radzą sobie w słabym świetle i oferują szerszy zakres tonalny. Wybór aparatu z określoną wielkością matrycy i liczbą pikseli to zatem kompromis. Więcej megapikseli na małej matrycy oznacza mniejsze piksele, co może negatywnie wpłynąć na jakość obrazu w trudnych warunkach, mimo wyższej teoretycznej rozdzielczości.\nWarto również zwrócić uwagę na percepcję \u0026ldquo;crop factora\u0026rdquo; i jego wpływu na głębię ostrości. Współczynnik kadrowania wynika z mniejszego rozmiaru matrycy, która \u0026ldquo;wycina\u0026rdquo; fragment obrazu rzutowanego przez obiektyw. Aby uzyskać ten sam kadr (to samo pole widzenia) na matrycy z crop factorem, trzeba użyć obiektywu o krótszej ogniskowej. Głębia ostrości zależy od przysłony, odległości od obiektu i rzeczywistej ogniskowej. Przy tej samej liczbie f i tym samym kadrze, mniejsza matryca da większą głębię ostrości. Dzieje się tak, ponieważ krótsza ogniskowa sama w sobie daje większą DOF. Aby uzyskać tę samą głębię ostrości co na pełnej klatce, na mniejszej matrycy trzeba by użyć proporcjonalnie szerszego otworu przysłony. Zrozumienie tego niuansu jest kluczowe dla świadomego wyboru sprzętu i technik fotografowania, szczególnie gdy celem jest specyficzna kontrola nad głębią ostrości.\n"},{"id":6,"href":"/photography-basics/docs/przyslona/","title":"Przysłona","section":"Docs","content":" Przysłona # Przysłona jest jednym z kluczowych elementów układu optycznego aparatu fotograficznego, pełniącym rolę analogiczną do źrenicy w ludzkim oku. Jej głównym zadaniem jest regulacja ilości światła przechodzącego przez obiektyw i docierającego do matrycy światłoczułej, ale ma ona również fundamentalny wpływ na inne aspekty obrazu, takie jak głębia ostrości i charakter rozmycia tła (bokeh).\nMechanizm przysłony irysowej # Większość obiektywów fotograficznych wyposażona jest w tzw. przysłonę irysową.\nBudowa: Przysłona irysowa składa się z zestawu cienkich, zachodzących na siebie metalowych blaszek, nazywanych listkami lub lamelami. Te listki ułożone są w okrąg i mogą się poruszać, zmieniając średnicę centralnego otworu, przez który przechodzi światło. Kształt i liczba listków: Liczba listków (np. od 5 do 11 lub więcej) oraz ich kształt (proste krawędzie lub zaokrąglone) mają istotny wpływ na wygląd nieostrych punktów światła w tle, czyli efekt bokeh. Obiektywy z większą liczbą listków i/lub listkami o zaokrąglonych krawędziach tworzą bardziej okrągły otwór, co przekłada się na gładsze i przyjemniejsze dla oka rozmycie oraz bardziej okrągłe bliki świetlne. Sposób regulacji średnicy otworu: Średnica otworu przysłony jest precyzyjnie kontrolowana przez mechanizm wewnątrz obiektywu. W starszych konstrukcjach regulacja odbywała się manualnie za pomocą pierścienia na obiektywie. W nowoczesnych aparatach odbywa się to najczęściej elektronicznie, poprzez sygnały sterujące wysyłane z korpusu aparatu, które poruszają listkami przysłony do żądanej pozycji. Wpływ otworu przysłony na obraz # Zmiana wielkości otworu przysłony ma dwojaki, fundamentalny wpływ na finalny obraz: reguluje ilość światła docierającego do matrycy oraz modyfikuje głębię ostrości.\nRegulacja ilości światła docierającego do matrycy # Jest to podstawowa i najbardziej intuicyjna funkcja przysłony. Zgodnie z zasadą działania, większy otwór przysłony (odpowiadający mniejszej wartości liczbowej f, np. f/1.4, f/2) przepuszcza więcej światła do wnętrza aparatu w jednostce czasu. Z kolei mniejszy otwór przysłony (odpowiadający większej wartości liczbowej f, np. f/16, f/22) ogranicza ilość przepuszczanego światła. Przysłona, obok czasu naświetlania i czułości ISO, stanowi jeden z trzech filarów tzw. trójkąta ekspozycji, pozwalając fotografowi na precyzyjne kontrolowanie jasności zdjęcia.1\nGłębia ostrości (DOF – Depth of Field) # Głębia ostrości to jeden z najważniejszych kreatywnych narzędzi w fotografii, a jej kontrola jest nierozerwalnie związana z ustawieniem przysłony.\nDefinicja i percepcja ostrości: Głębia ostrości (DOF) definiowana jest jako zakres odległości, mierzony wzdłuż osi optycznej obiektywu, w którym obiekty na zdjęciu wydają się być \u0026ldquo;akceptowalnie ostre\u0026rdquo; dla ludzkiego oka. Należy podkreślić, że idealnie ostry jest tylko jeden plan – ten, na który dokładnie ustawiono ostrość. Obiekty znajdujące się bliżej lub dalej od tego planu są w rzeczywistości w pewnym stopniu rozmyte. Głębia ostrości opisuje więc strefę, w której to rozmycie jest na tyle małe, że nie jest dostrzegane jako nieostrość. Optyka geometryczna głębi ostrości: krążek rozproszenia (Circle of Confusion – CoC):\nAby zrozumieć, dlaczego zmiana przysłony wpływa na głębię ostrości, należy wprowadzić pojęcie krążka rozproszenia (CoC). Idealny obiektyw skupia wszystkie promienie światła wychodzące z jednego punktu na obiekcie w jednym punkcie na płaszczyźnie matrycy (lub filmu) – pod warunkiem, że ten punkt obiektu leży w płaszczyźnie ostrości. Jeśli jednak punkt obiektu znajduje się przed lub za płaszczyzną ostrości, promienie światła po przejściu przez obiektyw nie zbiegną się idealnie w jednym punkcie na matrycy, lecz utworzą na niej niewielką, rozmytą plamkę. Kształt tej plamki jest rzutem kształtu otworu przysłony, a dla uproszczenia przyjmuje się, że jest to koło – stąd nazwa \u0026ldquo;krążek rozproszenia\u0026rdquo;. \u0026ldquo;Akceptowalna wielkość CoC\u0026rdquo; to maksymalny rozmiar takiego krążka rozproszenia na powierzchni matrycy, który na finalnym obrazie (np. odbitce oglądanej z typowej odległości) będzie wciąż postrzegany przez ludzkie oko jako ostry punkt, a nie jako rozmyta plamka. Wartość ta zależy od wielu czynników, m.in. od rozmiaru matrycy, rozdzielczości, zamierzonego powiększenia obrazu oraz odległości i warunków jego oglądania. Dla matryc pełnoklatkowych (36x24 mm) często przyjmuje się standardową wartość CoC w granicach 0.5 mm do 0.0 mm. Wpływ wartości przysłony na rozmiar krążka rozproszenia i głębię ostrości: Duży otwór przysłony (mała liczba f, np. f/1.4, f/2.8): Gdy przysłona jest szeroko otwarta, promienie światła pochodzące z punktów obiektu znajdujących się poza płaszczyzną ostrości tworzą stosunkowo szeroki stożek, który przecina płaszczyznę matrycy, tworząc na niej duże krążki rozproszenia. W rezultacie, tylko obiekty znajdujące się bardzo blisko płaszczyzny ostrości będą odwzorowane jako akceptowalnie ostre (ich CoC będzie mniejszy od limitu). Głębia ostrości jest wówczas mała (płytka). Jest to pożądane np. w fotografii portretowej, gdzie chcemy wyizolować ostry obiekt od rozmytego tła. Mały otwór przysłony (duża liczba f, np. f/11, f/16): Gdy przysłona jest mocno domknięta, stożek promieni światła jest znacznie węższy. Nawet dla punktów obiektu znacznie oddalonych od płaszczyzny ostrości, krążki rozproszenia tworzone na matrycy są małe. W efekcie, szeroki zakres odległości (zarówno przed, jak i za płaszczyzną ostrości) będzie odwzorowany z akceptowalną ostrością. Głębia ostrości jest wówczas duża (głęboka). Jest to wykorzystywane np. w fotografii krajobrazowej, gdzie dąży się do uzyskania ostrości na wszystkich planach, od pierwszego planu po horyzont. Wizualizacje tego zjawiska, często przedstawiane jako diagramy promieni (np. 28), pokazują, jak zmiana średnicy otworu przysłony wpływa na kąt zbieżności stożka promieni, a tym samym na rozmiar krążka rozproszenia dla obiektów nie znajdujących się w płaszczyźnie ostrości. Wpływ odległości fotografowanego obiektu od aparatu: Im bliżej znajduje się obiekt, na który ustawiona jest ostrość, tym mniejsza jest głębia ostrości, nawet przy tej samej wartości przysłony i ogniskowej obiektywu. Jest to szczególnie widoczne w makrofotografii, gdzie głębia ostrości może wynosić zaledwie ułamki milimetra. Wynika to z faktu, że przy małych odległościach ostrzenia, względna różnica odległości do obiektów tła i pierwszego planu staje się bardzo duża, a kąty, pod jakimi promienie z tych obiektów wpadają do obiektywu, są bardziej zróżnicowane, co prowadzi do szybszego wzrostu rozmiaru krążków rozproszenia dla punktów poza płaszczyzną ostrości. Wpływ odległości obiektu od tła: Im większa jest odległość między ostro sfotografowanym obiektem a tłem, tym bardziej rozmyte będzie tło, ponieważ znajdzie się ono dalej poza (i tak już płytką, jeśli używamy dużej przysłony i/lub długiej ogniskowej) strefą głębi ostrości. Wpływ ogniskowej obiektywu: Przy tej samej wartości przysłony i tej samej odległości od fotografowanego obiektu, obiektywy o dłuższej ogniskowej generalnie dają mniejszą (płytszą) głębię ostrości. Dłuższe ogniskowe powodują większe powiększenie tła, w tym również jego rozmycia. Nawet jeśli geometryczny rozmiar krążków rozproszenia na matrycy byłby podobny, ich większe powiększenie w finalnym obrazie sprawia wrażenie płytszej głębi ostrości. Rozkład głębi ostrości (zasada 1/3 przed, 2/3 za punktem ostrości): Głębia ostrości nie rozkłada się symetrycznie wokół płaszczyzny, na którą ustawiono ostrość. Zazwyczaj około jedna trzecia głębi ostrości znajduje się przed płaszczyzną ostrości, a dwie trzecie za nią. Ten stosunek nie jest jednak stały i zmienia się w zależności od odległości ostrzenia i ogniskowej obiektywu. Przy bardzo małych odległościach ostrzenia (np. w makrofotografii) rozkład głębi ostrości jest bliższy symetrii 1:1. Natomiast przy ostrzeniu na tzw. odległość hiperfokalną, głębia ostrości rozciąga się od około połowy tej odległości aż do nieskończoności. Zrozumienie tego niesymetrycznego rozkładu jest istotne dla precyzyjnego ustawiania ostrości, np. w fotografii portretowej (gdzie ostrość ustawia się na oczy, licząc, że nos i uszy również znajdą się w strefie akceptowalnej ostrości) lub w fotografii krajobrazowej. Praktyczne zastosowanie kontroli głębi ostrości: Możliwość manipulowania głębią ostrości jest jednym z podstawowych narzędzi ekspresji w fotografii. Mała głębia ostrości (uzyskiwana przy dużych otworach przysłony, np. f/1.8, f/2.8) pozwala na wyizolowanie głównego obiektu od tła, skupiając na nim uwagę widza – jest to często stosowane w fotografii portretowej, produktowej czy sportowej. Duża głębia ostrości (uzyskiwana przy małych otworach przysłony, np. f/11, f/16) zapewnia ostrość na wielu planach zdjęcia, co jest pożądane w fotografii krajobrazowej, architektury czy dokumentalnej, gdzie ważne jest pokazanie kontekstu i szczegółów otoczenia. Głębia ostrości jest zjawiskiem nie tylko geometrycznym, ale również percepcyjnym. Definicja \u0026ldquo;akceptowalnej ostrości\u0026rdquo; i dopuszczalnego rozmiaru krążka rozproszenia zależy od warunków oglądania finalnego obrazu, jego wielkości (np. mała odbitka vs duży wydruk wystawowy) oraz indywidualnej percepcji widza. Optyka geometryczna dostarcza narzędzi do obliczenia rozmiaru krążków rozproszenia na matrycy, ale to, czy dany krążek zostanie uznany za \u0026ldquo;ostry punkt\u0026rdquo; czy \u0026ldquo;rozmytą plamkę\u0026rdquo;, zależy od tego, jak duży będzie on na finalnym obrazie i z jakiej odległości będzie oglądany. Mniejszy wydruk lub większa odległość oglądania pozwalają na większy akceptowalny CoC na matrycy. Dlatego głębia ostrości nie jest wartością absolutną, lecz zależy od kontekstu użytkowania zdjęcia. Fotografowie muszą brać pod uwagę nie tylko ustawienia aparatu, ale także przeznaczenie zdjęcia, aby świadomie kontrolować głębię ostrości.\nEfekt bokeh # Termin \u0026ldquo;bokeh\u0026rdquo; pochodzi z języka japońskiego i odnosi się do estetycznej jakości rozmycia nieostrych partii obrazu, a w szczególności sposobu, w jaki odwzorowywane są nieostre punkty światła (tzw. bliki). Nie chodzi tu jedynie o stopień rozmycia (który zależy od głębi ostrości), ale o jego charakter – czy jest ono gładkie, \u0026ldquo;kremowe\u0026rdquo;, czy może \u0026ldquo;nerwowe\u0026rdquo;, z wyraźnymi krawędziami.\nOptyczne podstawy powstawania efektu bokeh: Rola kształtu listków przysłony: Kształt otworu przysłony ma bezpośredni i najbardziej widoczny wpływ na kształt krążków rozproszenia (blików) tworzonych przez nieostre punkty światła. Gdy przysłona jest maksymalnie otwarta, otwór jest zazwyczaj zbliżony do koła, a bliki są okrągłe. Po przymknięciu przysłony, otwór przybiera kształt wielokąta utworzonego przez listki przysłony, a bliki odwzorowują ten kształt. Obiektywy z większą liczbą listków (np. 9, 11) i listkami o zaokrąglonych krawędziach są w stanie utrzymać bardziej okrągły kształt otworu nawet po przymknięciu, co prowadzi do powstawania bardziej okrągłych i często uważanych za przyjemniejsze dla oka blików bokeh. Wpływ konstrukcji obiektywu i korekcji aberracji: Jakość i charakter bokeh zależą również w dużym stopniu od ogólnej konstrukcji optycznej obiektywu, a w szczególności od sposobu skorygowania różnych aberracji optycznych. Kluczową rolę odgrywa tu aberracja sferyczna. Aberracja sferyczna polega na tym, że promienie światła przechodzące przez różne strefy soczewki (centralną i brzegowe) nie są skupiane w jednym punkcie. Sposób, w jaki ta aberracja jest skorygowana (lub celowo niedokorygowana), wpływa na rozkład jasności wewnątrz krążka rozproszenia. Na przykład, niedokorygowana aberracja sferyczna może prowadzić do powstawania blików z jaśniejszym środkiem i miękkimi krawędziami (tzw. \u0026ldquo;gładki\u0026rdquo; lub \u0026ldquo;mydlany\u0026rdquo; bokeh), podczas gdy przekorygowana aberracja sferyczna może skutkować blikami z jasnymi, ostrymi krawędziami i ciemniejszym środkiem (tzw. \u0026ldquo;nerwowy\u0026rdquo; bokeh). Inne aberracje, takie jak koma czy astygmatyzm, również mogą wpływać na kształt blików, szczególnie w brzegowych partiach kadru, prowadząc np. do powstawania blików w kształcie \u0026ldquo;kocich oczu\u0026rdquo;. Niektóre historyczne lub specjalistyczne konstrukcje obiektywów, jak np. obiektywy Petzvala, są znane z bardzo charakterystycznego, \u0026ldquo;wirującego\u0026rdquo; efektu bokeh (ang. swirly bokeh), który jest wynikiem specyficznej kombinacji aberracji. Jak kąt padania promieni światła z nieostrych punktów na matrycę, w zależności od otworu przysłony, tworzy charakterystyczne krążki rozproszenia: Promienie światła wychodzące z punktu znajdującego się poza głębią ostrości, po przejściu przez otwór przysłony, nie skupiają się w jednym punkcie na płaszczyźnie matrycy, lecz tworzą na niej plamkę świetlną. Kształt tej plamki jest wiernym odwzorowaniem kształtu otworu przysłony. Wielkość tej plamki (czyli stopień rozmycia) zależy od tego, jak bardzo dany punkt jest oddalony od płaszczyzny ostrości, oraz od wielkości otworu przysłony – im większy otwór i im dalej punkt od płaszczyzny ostrości, tym większy krążek rozproszenia. Bokeh jest często uważany za \u0026ldquo;charakter\u0026rdquo; lub \u0026ldquo;duszę\u0026rdquo; obiektywu, odzwierciedlający specyficzne wybory projektowe i kompromisy dokonane przez jego konstruktorów. Różne obiektywy, nawet o tej samej ogniskowej i jasności, mogą produkować bokeh o zupełnie innym charakterze. Wynika to z faktu, że idealny obiektyw nie istnieje, a każda konstrukcja optyczna jest wynikiem kompromisów między minimalizacją różnych aberracji, ostrością, kontrastem, a także rozmiarem, wagą i ceną. Wybierając obiektyw, szczególnie do zastosowań portretowych czy artystycznych, warto zwracać uwagę nie tylko na jego parametry techniczne, takie jak ostrość, ale także na charakter generowanego przez niego bokeh, który może znacząco wpłynąć na ostateczną estetykę zdjęć.\n"},{"id":7,"href":"/photography-basics/docs/fstops/","title":"Dlaczego światło liczy się w przysłonach (f-stops)","section":"Docs","content":"Zrozumienie systemu wartości przysłony (f-stopów) oraz ich związku z czasem naświetlania i czułością ISO jest fundamentalne dla świadomego kontrolowania ekspozycji w fotografii. System ten, choć na pierwszy rzut oka może wydawać się nieintuicyjny, opiera się na solidnych podstawach fizycznych i matematycznych, które zapewniają jego uniwersalność i precyzję.\nDefinicja liczby f (f-stop) # Wartość przysłony, powszechnie określana jako liczba f lub f-stop, jest kluczowym parametrem opisującym, jak dużo światła przepuszcza obiektyw.\nStosunek ogniskowej obiektywu (f) do średnicy czynnej otworu przysłony (D): Liczba f (oznaczana jako N) jest definiowana jako stosunek ogniskowej obiektywu (f) do efektywnej średnicy otworu przysłony (D). Matematycznie wyraża się to wzorem: N=f/D Z tego wzoru wynika, że średnica otworu przysłony D=f/N.\nDlaczego mniejsza liczba f oznacza większy otwór i więcej światła: Ponieważ liczba f (N) znajduje się w mianowniku wyrażenia na średnicę (lub średnica D jest w mianowniku wzoru N = f/D), mniejsza wartość liczbowa N (np. f/1.4, f/2) oznacza większą efektywną średnicę otworu przysłony D. Większa średnica otworu to z kolei większa jego powierzchnia, przez którą może wpadać światło do wnętrza aparatu. Można to przyrównać do ułamków: 1/2 jest wartością większą niż 1/16, stąd przysłona f/2 wpuszcza więcej światła niż f/16.\nSkala wartości f-stop i jej związek z ilością światła # Standardowy szereg wartości przysłony został tak skonstruowany, aby każda kolejna pełna wartość (stopień) zmieniała ilość przepuszczanego światła dokładnie dwukrotnie.\nStandardowy szereg f-stopów: Typowe pełne stopnie przysłony, które można znaleźć na obiektywach lub w ustawieniach aparatu, to: f/1, f/1.4, f/2, f/2.8, f/4, f/5.6, f/8, f/11, f/16, f/22, f/32, itd.. Współczesne aparaty cyfrowe pozwalają na bardziej precyzyjną regulację przysłony, zazwyczaj w krokach co 1/2 lub 1/3 stopnia EV (wartości ekspozycji).\nJak zmiana przysłony wpływa na ilość światła? Wyjaśnienie # Kluczem do zrozumienia, dlaczego każdy kolejny stopień przysłony (f-stop) zmienia ilość światła dwukrotnie, jest zrozumienie zależności między trzema elementami: powierzchnią otworu, jego średnicą oraz liczbą f.\nKrok 1: Liczy się powierzchnia, nie średnica # Ilość światła, która wpada do aparatu, jest wprost proporcjonalna do powierzchni otworu przysłony. Otwór ten jest kołem, a pole powierzchni koła (A) obliczamy ze wzoru, używając jego średnicy (D):\nA = π * (D/2)² = (π * D²) / 4\nAby podwoić ilość wpadającego światła, musimy podwoić powierzchnię otworu (A₂ = 2 * A₁). Zobaczmy, jak zmiana ta wpływa na wymaganą średnicę (D):\n(π * D₂²) / 4 = 2 * (π * D₁²) / 4\nPo uproszczeniu wzoru przez usunięcie z obu stron π oraz /4 otrzymujemy:\nD₂² = 2 * D₁²\nA po wyciągnięciu pierwiastka z obu stron:\nD₂ = D₁ * √2\nWniosek: Aby podwoić powierzchnię otworu (i wpuścić dwa razy więcej światła), jego średnica musi wzrosnąć o czynnik √2 (czyli około 1,41).\nKrok 2: Czym jest liczba f (f-number)? # Liczba f (oznaczana jako N) to stosunek ogniskowej obiektywu (f) do średnicy otworu przysłony (D):\nN = ogniskowa (f) / średnica (D)\nZ tego wzoru wynika, że liczba f jest odwrotnie proporcjonalna do średnicy otworu.\nWiększa średnica (więcej światła) = mniejsza liczba f (np. f/1.4). Mniejsza średnica (mniej światła) = większa liczba f (np. f/16). Krok 3: Łączymy wszystko w całość # Skoro wiemy już wszystko, połączmy te fakty:\nAby zmniejszyć ilość światła o połowę (czyli zamknąć przysłonę o 1 stopień, np. z f/2.8 na f/4), musimy zmniejszyć powierzchnię dwukrotnie. Wymaga to zmniejszenia średnicy o czynnik √2. Aby to osiągnąć, liczba f musi zostać pomnożona przez √2.\nPrzykład: f/2.8 * √2 ≈ f/4 Aby zwiększyć ilość światła dwukrotnie (czyli otworzyć przysłonę o 1 stopień, np. z f/4 na f/2.8), musimy zwiększyć powierzchnię dwukrotnie. Wymaga to zwiększenia średnicy o czynnik √2. Aby to osiągnąć, liczba f musi zostać podzielona przez √2.\nPrzykład: f/4 / √2 ≈ f/2.8 Standardowy szereg wartości przysłony # Dlatego właśnie standardowy szereg wartości przysłony jest tworzony przez kolejne mnożenie (lub dzielenie) przez pierwiastek z dwóch (≈ 1.4).\nf/1, f/1.4, f/2, f/2.8, f/4, f/5.6, f/8, f/11, f/16, f/22\nKażdy kolejny krok w prawo w tym szeregu oznacza zmniejszenie ilości światła o połowę. Każdy krok w lewo oznacza jej podwojenie.\nWartość Ekspozycji (EV – Exposure Value) # Wartość Ekspozycji (EV) to ustandaryzowana skala używana w fotografii do określenia ilości światła.\nDefinicja EV: EV jest skalą logarytmiczną o podstawie 2. Służy do opisania kombinacji ustawień czasu naświetlania i wartości przysłony, które dają ten sam poziom naświetlenia matrycy, lub do określenia luminancji (jasności) fotografowanej sceny. W praktyce, światłomierz aparatu często wskazuje odchyłkę od \u0026ldquo;poprawnej\u0026rdquo; ekspozycji w jednostkach EV; zdjęcie idealnie naświetlone według wskazań światłomierza ma umownie 0 EV. Jak zmiana o 1 EV odpowiada dwukrotnej zmianie ilości światła: Zmiana ekspozycji o +1 EV oznacza dwukrotne zwiększenie ilości światła docierającego do matrycy (np. zdjęcie staje się jaśniejsze). Zmiana o -1 EV oznacza dwukrotne zmniejszenie ilości światła (zdjęcie staje się ciemniejsze). Każda zmiana wartości przysłony o jeden pełny stopień, czasu naświetlania o jeden pełny stopień (np. z 1/125 s na 1/250 s lub odwrotnie) lub czułości ISO o jeden pełny stopień (np. z ISO 100 na ISO 200 lub odwrotnie) odpowiada zmianie ekspozycji o 1 EV. Trójkąt ekspozycji # Ekspozycja zdjęcia, czyli całkowita ilość światła zarejestrowana przez matrycę, jest determinowana przez trzy wzajemnie powiązane parametry: wartość przysłony (f-stop), czas naświetlania (długość otwarcia migawki) oraz czułość ISO matrycy.1 Te trzy elementy tworzą tzw. \u0026ldquo;trójkąt ekspozycji\u0026rdquo;.\nWzajemne zależności między przysłoną, czasem naświetlania i czułością ISO: Aby uzyskać prawidłowo naświetlone zdjęcie (czyli dostarczyć do matrycy odpowiednią ilość światła), fotograf musi zbalansować te trzy ustawienia. Zmiana jednego z nich wpływa na wymaganą wartość pozostałych. Jak zmiana jednego parametru o określoną liczbę stopni EV musi być skompensowana zmianą innego parametru, aby utrzymać ten sam poziom ekspozycji: Kluczową zasadą jest to, że aby zachować ten sam całkowity poziom naświetlenia (to samo EV), zwiększenie ilości światła przez jeden parametr (np. otwarcie przysłony o 1 EV) musi być skompensowane zmniejszeniem ilości światła przez inny parametr o tę samą wartość EV (np. skrócenie czasu naświetlania o 1 EV lub zmniejszenie ISO o 1 EV). Na przykład, jeśli przymkniemy przysłonę o jeden stopień (np. z f/2.8 na f/4), co zmniejsza ilość światła o połowę (-1 EV), musimy, aby zachować tę samą ekspozycję: albo dwukrotnie wydłużyć czas naświetlania (np. z 1/500 s na 1/250 s, co daje +1 EV), albo dwukrotnie zwiększyć czułość ISO (np. z ISO 100 na ISO 200, co daje +1 EV), albo zastosować kombinację zmian czasu i ISO, która sumarycznie da +1 EV. Praktyczne przykłady przeliczeń: 44 Załóżmy, że prawidłowa ekspozycja dla danej sceny to ISO 200, f/8, 1/125 s. Jeśli chcemy uzyskać mniejszą głębię ostrości i otwieramy przysłonę do f/4 (co oznacza +2 EV, bo f/8 -\u0026gt; f/5.6 -\u0026gt; f/4), musimy zmniejszyć ilość światła o 2 EV za pomocą czasu i/lub ISO. Możemy np. skrócić czas do 1/500 s (1/125s -\u0026gt; 1/250s -\u0026gt; 1/500s, czyli -2 EV), pozostawiając ISO 200. Nowe ustawienia to ISO 200, f/4, 1/500 s. Jeśli chcemy zamrozić bardzo szybki ruch i potrzebujemy czasu 1/1000 s (z 1/125 s to zmiana o -3 EV, bo 1/125s -\u0026gt; 1/250s -\u0026gt; 1/500s -\u0026gt; 1/1000s), a przysłona f/8 ma pozostać bez zmian, musimy zwiększyć ISO o 3 EV (ISO 200 -\u0026gt; 400 -\u0026gt; 800 -\u0026gt; 1600). Nowe ustawienia to ISO 1600, f/8, 1/1000 s. Poniższa tabela ilustruje przykłady ekwiwalentnych ekspozycji, dających ten sam poziom naświetlenia (to samo EV), ale różne efekty wizualne:\nCel / Efekt Artystyczny ISO Przysłona (f-stop) Czas naświetlania (s) Przybliżone EV Standardowa ekspozycja (przykład) 200 f/8 1/125 ~13 Mniejsza głębia ostrości 200 f/4 1/500 ~13 (otwarcie przysłony o 2 EV, (+2 EV) (-2 EV) skrócenie czasu o 2 EV) Większa głębia ostrości 200 f/16 1/30 ~13 (przymknięcie przysłony o 2 EV, (-2 EV) (+2 EV) wydłużenie czasu o 2 EV) Zamrożenie ruchu 800 f/8 1/500 ~13 (skrócenie czasu o 2 EV, (+2 EV) (-2 EV) zwiększenie ISO o 2 EV) Rozmycie ruchu (np. woda) 100 f/22 1/4 ~13 (zmniejszenie ISO o 1 EV, (-1 EV) (-3 EV) (+4 EV) przymknięcie przysłony o 3 EV, znaczne wydłużenie czasu o 4 EV) Standaryzacja systemu f-stopów, oparta na czynniku 2​, jest niezwykle istotna, ponieważ tworzy uniwersalny język opisu ekspozycji, niezależny od konkretnego obiektywu, jego fizycznej wielkości czy ogniskowej. F-stop normalizuje średnicę otworu względem ogniskowej, a użycie skali opartej na 2​ gwarantuje, że zmiana o jeden pełny stopień f zawsze oznacza dwukrotną zmianę ilości światła. Dzięki temu fotograf może przewidywalnie kontrolować ekspozycję, nawet zmieniając obiektywy – f/2.8 na jednym obiektywie wpuszcza tyle samo światła co f/2.8 na innym (pomijając niewielkie różnice w transmisji światła przez soczewki, które opisuje bardziej precyzyjna, ale rzadziej stosowana skala T-stop). Ta standaryzacja jest fundamentem wymienności obiektywów i umożliwia stosowanie uniwersalnych zasad ekspozycji oraz zewnętrznych światłomierzy.\nCo więcej, logarytmiczna natura skali EV (i powiązanych z nią skal przysłony, czasu i ISO) dobrze koresponduje ze sposobem, w jaki ludzkie oko postrzega zmiany jasności. Ludzka percepcja jasności nie jest liniowa; abyśmy postrzegali równomierne przyrosty jasności, fizyczna ilość światła musi rosnąć geometrycznie (co opisuje prawo Webera-Fechnera w psychofizyce). Skala EV, gdzie każdy stopień oznacza podwojenie ilości światła, jest więc bardziej intuicyjna dla fotografa i upraszcza myślenie o ekspozycji. Zamiast operować na bezwzględnych wartościach luminancji, fotografowie mogą myśleć w kategoriach \u0026ldquo;stopni\u0026rdquo; lub \u0026ldquo;działek\u0026rdquo; ekspozycji, co ułatwia szybkie i precyzyjne dostosowywanie ustawień.\n"},{"id":8,"href":"/photography-basics/docs/winietowanie/","title":"Winietowanie","section":"Docs","content":" Winietowanie # Winietowanie jest częstym zjawiskiem w fotografii, objawiającym się jako stopniowy spadek jasności lub nasycenia kolorów na brzegach i w rogach kadru w porównaniu do jego centralnej części. Może być ono postrzegane jako wada optyczna obiektywu, ale czasami jest również stosowane jako celowy efekt artystyczny, mający na celu skupienie uwagi widza na środku obrazu. Zrozumienie przyczyn różnych typów winietowania jest kluczowe dla jego minimalizacji lub świadomego wykorzystania.\nRodzaje winietowania i ich szczegółowe przyczyny # Wyróżnia się kilka głównych rodzajów winietowania, z których każdy ma inne podłoże fizyczne.\nWinietowanie optyczne (naturalne) # Ten typ winietowania jest nieodłącznie związany z fizyką przechodzenia światła przez układ optyczny obiektywu i jest szczególnie widoczny w obiektywach szerokokątnych oraz jasnych (o dużym otworze względnym) przy maksymalnie otwartej przysłonie.\nFizyczne przyczyny: Prawo cos⁴θ (spadek natężenia oświetlenia dla promieni skośnych): Jest to najbardziej fundamentalna przyczyna winietowania naturalnego. Wynika ona z faktu, że promienie światła padające na brzegi matrycy (lub filmu) docierają do niej pod pewnym kątem (θ) względem osi optycznej, w przeciwieństwie do promieni padających na centrum matrycy, które biegną niemal prostopadle. Istnieją trzy główne efekty składające się na to prawo: Efektywna powierzchnia zbierająca światło przez piksel na brzegu matrycy jest mniejsza o czynnik cos(θ) w porównaniu do piksela w centrum (dla tej samej wiązki światła). Odległość od źrenicy wyjściowej obiektywu do brzegu matrycy jest większa niż do centrum o czynnik 1/cos(θ), co zgodnie z prawem odwrotnych kwadratów dla natężenia światła, zmniejsza natężenie oświetlenia o czynnik cos²(θ). Efektywny rozmiar źrenicy wyjściowej obiektywu, widziany z punktu na brzegu matrycy, może być mniejszy o czynnik cos(θ) (tzw. efekt \u0026ldquo;pupil compression\u0026rdquo; lub \u0026ldquo;vignetting by the aperture stop itself\u0026rdquo;). Połączenie tych efektów prowadzi do spadku natężenia oświetlenia na brzegach matrycy proporcjonalnego do cos⁴(θ). Konstrukcja obiektywu: Winietowanie optyczne wynika również z samej konstrukcji obiektywu. Soczewki mają ograniczoną średnicę, a promienie światła przechodzące przez ich zewnętrzne części (szczególnie przedniej i tylnej grupy soczewek) mogą być częściowo \u0026ldquo;przycinane\u0026rdquo; lub absorbowane przez oprawy poszczególnych soczewek lub przez samą obudowę obiektywu, zanim dotrą do płaszczyzny matrycy. Jest to szczególnie problematyczne w przypadku jasnych obiektywów szerokokątnych, gdzie kąty padania światła są duże. Wpływ otworu przysłony: Winietowanie optyczne jest zazwyczaj najbardziej widoczne przy maksymalnie otwartej przysłonie. Przymykanie przysłony (zwiększanie liczby f) często znacząco redukuje ten typ winietowania. Dzieje się tak, ponieważ mniejszy otwór przysłony ogranicza wiązkę światła do bardziej centralnej części soczewek, gdzie transmisja światła jest bardziej równomierna i mniej podatna na ograniczenia wynikające ze średnicy soczewek. Całkowite wyeliminowanie winietowania optycznego w fazie projektowania obiektywu, szczególnie szerokokątnego i jasnego, wymagałoby zastosowania soczewek o bardzo dużej średnicy i skomplikowanej konstrukcji, co prowadziłoby do znacznego wzrostu rozmiaru, wagi i ceny obiektywu. Dlatego konstruktorzy często idą na kompromis, akceptując pewien poziom winietowania, który może być później skorygowany cyfrowo.\nWinietowanie mechaniczne # Ten rodzaj winietowania powstaje, gdy fizyczne elementy, niebędące integralną częścią układu optycznego soczewek, blokują lub przesłaniają drogę światła na brzegach pola widzenia.\nPrzyczyny: Filtry fotograficzne: Stosowanie filtrów o zbyt grubych oprawkach, a zwłaszcza nakładanie kilku filtrów jeden na drugi (tzw. \u0026ldquo;stacking\u0026rdquo;), może powodować winietowanie mechaniczne, szczególnie w przypadku obiektywów szerokokątnych, gdzie pole widzenia jest bardzo szerokie. Osłony przeciwsłoneczne: Użycie nieodpowiedniej osłony przeciwsłonecznej (np. przeznaczonej do obiektywu o dłuższej ogniskowej na obiektywie szerokokątnym) lub jej nieprawidłowe zamocowanie (np. przekręcenie osłony tulipanowej) może prowadzić do zasłonięcia rogów kadru. Inne akcesoria: Winietowanie mechaniczne mogą również powodować inne akcesoria montowane na przedniej części obiektywu, takie jak niektóre adaptery do filtrów, konwertery szerokokątne lub telekonwertery, jeśli ich wewnętrzna średnica jest zbyt mała w stosunku do pola widzenia obiektywu. Charakterystyka: Winietowanie mechaniczne często objawia się jako gwałtowne, całkowite zaciemnienie rogów obrazu, czasami z wyraźnie widoczną, ostrą krawędzią przesłaniającego elementu. Wpływ przysłony na ten typ winietowania jest zazwyczaj mniejszy niż na winietowanie optyczne; jeśli coś fizycznie blokuje światło, przymknięcie przysłony może jedynie nieco zmienić kształt zaciemnionego obszaru. Winietowanie pikselowe (związane z sensorem) # Ten typ winietowania, nazywany również winietowaniem cyfrowym, występuje wyłącznie w aparatach cyfrowych i jest związany z konstrukcją samej matrycy światłoczułej oraz sposobem, w jaki piksele zbierają światło.\nPrzyczyny: Mikrosoczewki: Większość współczesnych matryc światłoczułych posiada warstwę mikrosoczewek umieszczonych nad każdym pikselem. Ich zadaniem jest skupienie jak największej ilości światła padającego na powierzchnię piksela na jego światłoczułym obszarze (fotodiodzie), co zwiększa efektywność zbierania światła, zwłaszcza w przypadku małych pikseli. Jednakże, efektywność tych mikrosoczewek może spadać dla promieni światła padających na matrycę pod dużym kątem (co ma miejsce na brzegach i w rogach sensora). Kątowa czułość pikseli: Same fotodiody, będące elementami światłoczułymi, wykazują największą czułość na światło padające prostopadle do ich powierzchni. Promienie światła padające pod znacznym kątem są absorbowane mniej efektywnie, co prowadzi do osłabienia sygnału generowanego przez piksele znajdujące się na brzegach matrycy. Ten efekt potęguje naturalne winietowanie optyczne. Wpływ konstrukcji obiektywu: Obiektywy o konstrukcji telecentrycznej po stronie obrazu (ang. image-space telecentric lenses) są zaprojektowane tak, aby główne promienie świetlne padały na całą powierzchnię matrycy możliwie jak najbardziej prostopadle. Takie obiektywy minimalizują winietowanie pikselowe. Są one częściej spotykane w systemach z mniejszymi matrycami (np. Mikro Cztery Trzecie) oraz w zastosowaniach przemysłowych i naukowych, gdzie równomierność oświetlenia jest krytyczna. Warto zauważyć, że obserwowane na zdjęciu winietowanie jest często sumą efektów różnych typów. Na przykład, naturalne winietowanie optyczne może być potęgowane przez winietowanie pikselowe. Jeśli dodatkowo użyjemy nieodpowiedniego filtra, nałoży się na to jeszcze winietowanie mechaniczne. Diagnozując problem, należy rozważyć wszystkie potencjalne przyczyny.\nMetody zapobiegania i korygowania winietowania # Istnieje szereg metod pozwalających na minimalizację winietowania na etapie fotografowania oraz na jego korekcję w procesie postprodukcji.\nNa etapie fotografowania # Dobór obiektywu: Niektóre obiektywy, szczególnie te wyższej klasy, są projektowane z myślą o minimalizacji winietowania. Przed zakupem warto zapoznać się z testami i recenzjami obiektywów, które często zawierają pomiary winietowania przy różnych wartościach przysłony. Przymykanie przysłony: Jest to najskuteczniejsza metoda redukcji winietowania optycznego i często również pikselowego. Zazwyczaj przymknięcie przysłony o 1-2 stopnie EV od wartości maksymalnej znacząco zmniejsza lub eliminuje ten problem. Odpowiednie akcesoria: Aby uniknąć winietowania mechanicznego, należy: Stosować filtry o cienkich oprawkach (typu \u0026ldquo;slim\u0026rdquo;), zwłaszcza przy obiektywach szerokokątnych. Unikać jednoczesnego stosowania wielu filtrów. Używać osłon przeciwsłonecznych dedykowanych do danego modelu obiektywu i prawidłowo je mocować. Format matrycy/obiektywu: Należy używać obiektywów przeznaczonych dla danego formatu matrycy. Na przykład, stosowanie obiektywu zaprojektowanego dla matrycy APS-C na aparacie pełnoklatkowym (bez włączonego trybu kadrowania w aparacie) spowoduje bardzo silne winietowanie (całkowite zaciemnienie brzegów kadru), ponieważ pole obrazowe takiego obiektywu jest zbyt małe, aby pokryć całą powierzchnię matrycy pełnoklatkowej. Techniki korygowania winietowania w postprodukcji # Jeśli winietowanie wystąpiło na zdjęciu, można je skorygować lub zredukować za pomocą oprogramowania do edycji zdjęć.\nAutomatyczna korekcja profilami obiektywów: Większość zaawansowanych programów do wywoływania plików RAW (takich jak Adobe Lightroom, Camera Raw, DxO PhotoLab, Capture One) posiada wbudowane profile korekcyjne dla szerokiej gamy obiektywów. Te profile zawierają informacje o charakterystyce danego obiektywu, w tym o jego typowym winietowaniu przy różnych ogniskowych i przysłonach. Zastosowanie takiego profilu pozwala na automatyczną i zazwyczaj bardzo skuteczną korekcję winietowania, a także innych wad optycznych, takich jak dystorsja czy aberracja chromatyczna. Manualna korekcja: Programy graficzne oferują również narzędzia do manualnej korekcji winietowania. Zazwyczaj są to suwaki pozwalające na rozjaśnienie brzegów kadru (suwak \u0026ldquo;Amount\u0026rdquo; lub \u0026ldquo;Siła\u0026rdquo;), określenie zasięgu korekcji (suwak \u0026ldquo;Midpoint\u0026rdquo; lub \u0026ldquo;Punkt środkowy\u0026rdquo;) oraz płynności przejścia między skorygowanym a nieskorygowanym obszarem (suwak \u0026ldquo;Feather\u0026rdquo; lub \u0026ldquo;Wtapianie\u0026rdquo;). Winietowanie jako efekt artystyczny: Czasami fotografowie celowo dodają winietowanie w postprodukcji (lub używają obiektywów generujących silne winietowanie), aby uzyskać określony efekt artystyczny. Delikatne przyciemnienie brzegów kadru może pomóc skupić uwagę widza na głównym motywie zdjęcia, dodać głębi lub nadać fotografii klimat retro. Poniższa tabela systematyzuje informacje o różnych typach winietowania:\nCecha Winietowanie Optyczne/Naturalne Winietowanie Mechaniczne Winietowanie Pikselowe/Cyfrowe Główne przyczyny Prawo cos⁴θ, ograniczenia konstrukcyjne soczewek (średnica, wewnętrzne przesłanianie) 60 Fizyczne blokowanie światła przez filtry, osłony przeciwsłoneczne, inne akcesoria 60 Kątowa czułość pikseli, efektywność mikrosoczewek dla światła padającego pod kątem 60 Charakterystyczne objawy Płynny spadek jasności od centrum ku brzegom kadru 60 Często gwałtowne zaciemnienie rogów, czasem z ostrymi krawędziami 60 Płynny spadek jasności, potęgujący winietowanie optyczne; zależny od konstrukcji matrycy i obiektywu (telecentryczność) 60 Wpływ przysłony Znacząca redukcja przy przymknięciu 60 Zwykle niewielki wpływ; może zmienić kształt zaciemnienia Może być zredukowane przez przymykanie przysłony (zmniejszenie kątów padania światła) Metody zapobiegania (fotografowanie) Przymykanie przysłony, wybór obiektywu o niskim winietowaniu 60 Stosowanie cienkich filtrów, unikanie stackowania, używanie dedykowanych osłon 61 Używanie obiektywów telecentrycznych (jeśli to możliwe i uzasadnione) Metody korekcji (postprodukcja) Profile obiektywów, manualna korekcja jasności brzegów 61 Kadrowanie zdjęcia, manualne rozjaśnianie (trudniejsze przy ostrych krawędziach) Profile obiektywów (często korygowane łącznie z optycznym), manualna korekcja "},{"id":9,"href":"/photography-basics/docs/dyfrakcja/","title":"Dyfrakcja światła w fotografii","section":"Docs","content":"Jednym z paradoksów w fotografii jest fakt, że choć przymykanie przysłony generalnie zwiększa głębię ostrości i może poprawiać ostrość poprzez redukcję niektórych aberracji optycznych, zbyt mocne jej domknięcie (np. do wartości f/16, f/22, f/32 i mniejszych) prowadzi do zauważalnego spadku ogólnej ostrości obrazu. Zjawiskiem odpowiedzialnym za ten efekt jest dyfrakcja światła.\nWprowadzenie do dyfrakcji światła # Natura falowa światła: Aby zrozumieć dyfrakcję, należy przypomnieć, że światło, oprócz tego, że składa się z cząstek (fotonów), wykazuje również właściwości falowe. Jako fala elektromagnetyczna, światło podlega zjawiskom typowym dla fal, takim jak interferencja i właśnie dyfrakcja. Czym jest dyfrakcja: Dyfrakcja to zjawisko fizyczne polegające na ugięciu się fali (w tym przypadku fali świetlnej) na krawędziach przeszkód lub podczas przechodzenia przez bardzo małe otwory. Zgodnie z zasadą Huygensa, każdy punkt na czole fali można traktować jako źródło nowej fali kulistej. Gdy fala napotyka przeszkodę lub wąską szczelinę, te wtórne fale interferują ze sobą, prowadząc do zmiany kierunku rozchodzenia się fali i jej \u0026ldquo;rozprzestrzeniania się\u0026rdquo; w obszarze cienia geometrycznego. Kiedy dyfrakcja staje się istotna w fotografii: Zjawisko dyfrakcji jest obecne zawsze, gdy światło przechodzi przez otwór przysłony w obiektywie. Jednak jego wpływ na jakość obrazu staje się zauważalny i zaczyna ograniczać ostrość dopiero wtedy, gdy otwór przysłony jest bardzo mały – czyli przy wysokich wartościach liczbowych f (np. f/16, f/22 i więcej). Dzieje się tak, ponieważ stopień ugięcia fali jest tym większy, im mniejszy jest rozmiar otworu w stosunku do długości fali światła. Przy małych otworach przysłony, ugięte fale świetlne zaczynają znacząco interferować, prowadząc do rozmycia obrazu. Wpływ dyfrakcji na jakość obrazu # Głównym i najbardziej niepożądanym skutkiem dyfrakcji w fotografii jest spadek postrzeganej ostrości i utrata drobnych detali na zdjęciu.\nRozmycie obrazu i spadek ostrości: Z powodu dyfrakcji, punktowe źródło światła (np. bardzo odległa gwiazda lub bardzo mały detal na fotografowanym obiekcie) nie jest już odwzorowywane na matrycy jako idealny punkt, nawet jeśli obiektyw jest doskonale skorygowany pod względem aberracji. Zamiast tego, obraz punktu staje się rozmytą plamką, znaną jako krążek Airy\u0026rsquo;ego, otoczoną serią coraz słabszych koncentrycznych pierścieni dyfrakcyjnych. To rozmycie powoduje, że drobne szczegóły obrazu zlewają się ze sobą, a ogólna ostrość zdjęcia ulega pogorszeniu. Jak dyfrakcja ogranicza zdolność rozdzielczą obiektywu: Zdolność rozdzielcza układu optycznego to jego zdolność do odróżniania dwóch blisko siebie położonych punktów lub linii. Dyfrakcja narzuca fundamentalne fizyczne ograniczenie na maksymalną możliwą do osiągnięcia zdolność rozdzielczą przez dany obiektyw przy określonym otworze przysłony. Gdy krążki Airy\u0026rsquo;ego odpowiadające dwóm bliskim punktom obiektu zaczynają się na siebie znacząco nakładać z powodu dyfrakcji, punkty te przestają być rozróżnialne na obrazie, co oznacza spadek zdolności rozdzielczej. Krążek Airy’ego # Pojęcie krążka Airy\u0026rsquo;ego jest kluczowe dla zrozumienia wpływu dyfrakcji na obrazowanie.\nObraz dyfrakcyjny punktu świetlnego: W idealnym układzie optycznym, którego jedynym ograniczeniem jest dyfrakcja (tzn. pozbawionym aberracji), obraz punktowego źródła światła utworzony przez okrągłą aperturę (jaką w przybliżeniu jest otwór przysłony) nie jest matematycznym punktem. Jest to charakterystyczny wzór dyfrakcyjny, składający się z centralnej, jasnej plamki zwanej dyskiem (lub krążkiem) Airy\u0026rsquo;ego, która zawiera około 84% całkowitej energii światła, oraz otaczających ją koncentrycznych, coraz słabszych jasnych i ciemnych pierścieni dyfrakcyjnych. Jak rozmiar krążka Airy’ego zależy od długości fali światła i wielkości otworu przysłony: Średnica pierwszego ciemnego pierścienia dyfrakcyjnego (który ogranicza dysk Airy\u0026rsquo;ego) jest dana wzorem: d≈2.⋅λ⋅(f/D) gdzie: d to średnica krążka Airy\u0026rsquo;ego na płaszczyźnie obrazu, λ to długość fali światła (np. dla światła zielonego ok. 550 nm), f to ogniskowa obiektywu, D to średnica otworu przysłony. Ponieważ N=f/D to liczba f (wartość przysłony), wzór można zapisać jako: d≈2.⋅λ⋅N Z tego wzoru wynikają dwie ważne zależności: Im mniejszy otwór przysłony (czyli większa liczba f, N), tym większy jest krążek Airy’ego. Oznacza to, że przy mocnym przymykaniu przysłony obraz punktu staje się coraz bardziej rozmyty przez dyfrakcję. Im dłuższa jest fala światła (np. światło czerwone ma dłuższą falę niż światło niebieskie), tym większy jest krążek Airy’ego dla tej samej wartości przysłony. Oznacza to, że dyfrakcja może być nieco bardziej widoczna dla światła czerwonego niż niebieskiego. Granica dyfrakcyjna rozdzielczości (kryterium Rayleigha): Aby dwa blisko siebie położone punkty świetlne mogły być rozróżnione na obrazie, ich krążki Airy\u0026rsquo;ego nie mogą się zbytnio na siebie nakładać. Zgodnie z kryterium Rayleigha, dwa punkty są uznawane za ledwo rozróżnialne, jeśli centrum krążka Airy\u0026rsquo;ego jednego punktu przypada na pierwszy ciemny pierścień dyfrakcyjny drugiego punktu. Gdy krążki Airy\u0026rsquo;ego są zbyt duże (z powodu małego otworu przysłony) i mocno się na siebie nakładają, detale obrazu zlewają się, a zdolność rozdzielcza systemu spada. Praktyczne konsekwencje dyfrakcji # Zrozumienie zjawiska dyfrakcji ma istotne implikacje praktyczne dla fotografów.\nDlaczego ekstremalne przymykanie przysłony (np. f/22, f/32) prowadzi do spadku ostrości, mimo teoretycznie dużej głębi ostrości: Jak wcześniej omówiono, przymykanie przysłony zwiększa głębię ostrości poprzez zmniejszenie rozmiaru geometrycznych krążków rozproszenia dla obiektów znajdujących się poza płaszczyzną ostrości. Jednakże, jednocześnie, zgodnie z prawami dyfrakcji, przymykanie przysłony prowadzi do powiększania się krążka Airy\u0026rsquo;ego dla każdego punktu obrazu. Przy bardzo małych otworach przysłony (wysokich liczbach f), efekt rozmycia dyfrakcyjnego staje się na tyle duży, że dominuje nad korzyściami płynącymi ze zwiększonej głębi ostrości i prowadzi do ogólnego spadku ostrości na całym zdjęciu, nawet w płaszczyźnie idealnego zogniskowania. Optymalna przysłona dla danego obiektywu (tzw. \u0026ldquo;sweet spot\u0026rdquo;): Każdy obiektyw fotograficzny charakteryzuje się pewnym zakresem wartości przysłony, przy którym osiąga on swoją najlepszą ostrość i ogólną jakość obrazu. Jest to tzw. \u0026ldquo;sweet spot\u0026rdquo; (optymalny punkt pracy). Jego istnienie wynika z kompromisu między dwoma przeciwstawnymi zjawiskami: Aberracje optyczne: Różnego rodzaju niedoskonałości układu optycznego (np. aberracja sferyczna, chromatyczna, koma, astygmatyzm) są zazwyczaj najbardziej widoczne przy maksymalnie otwartej przysłonie. Przymykanie przysłony o 1-3 stopnie EV często prowadzi do znacznej redukcji tych aberracji i poprawy ostrości, szczególnie na brzegach kadru. Dyfrakcja: Jak omówiono, wpływ dyfrakcji jest pomijalny przy dużych i średnich otworach przysłony, ale jej negatywny wpływ na ostrość gwałtownie narasta wraz z dalszym przymykaniem przysłony. \u0026ldquo;Sweet spot\u0026rdquo; znajduje się zazwyczaj w środkowym zakresie dostępnych wartości przysłony, np. f/5.6, f/8 lub f/11 dla wielu obiektywów pełnoklatkowych. Dokładna wartość optymalnej przysłony zależy od konkretnej konstrukcji obiektywu i można ją znaleźć w jego szczegółowych testach. Wpływ rozmiaru matrycy i gęstości pikseli na percepcję efektów dyfrakcji: Na matrycach o bardzo dużej gęstości pikseli (czyli o bardzo małych rozmiarach pojedynczych pikseli), negatywny wpływ dyfrakcji na ostrość może stać się widoczny już przy nieco większych otworach przysłony (czyli mniejszych liczbach f) niż na matrycach o tej samej wielkości fizycznej, ale mniejszej liczbie pikseli (czyli większych pikselach). Dzieje się tak, ponieważ rozmiar krążka Airy\u0026rsquo;ego może szybciej osiągnąć rozmiar porównywalny z rozmiarem kilku pikseli, co prowadzi do utraty informacji o drobnych detalach. W przypadku aparatów z mniejszymi matrycami (np. APS-C, Mikro Cztery Trzecie, czy matryce w smartfonach), które z natury używają obiektywów o krótszych ogniskowych i często mają bardzo małe piksele, dyfrakcja może stać się czynnikiem ograniczającym ostrość już przy umiarkowanych wartościach przysłony (np. f/8-f/11). Wiele aparatów w smartfonach ma stałą, zoptymalizowaną wartość przysłony, aby uniknąć problemów z dyfrakcją przy jednoczesnym zapewnieniu dużej głębi ostrości. Spadek ostrości spowodowany dyfrakcją przy małych otworach przysłony nie jest wynikiem wady czy złej jakości obiektywu, lecz fundamentalnym prawem fizyki falowej, któremu podlega światło. Każdy obiektyw, niezależnie od jego doskonałości i ceny, będzie wykazywał spadek ostrości spowodowany dyfrakcją przy dostatecznie małym otworze przysłony. Rozmiar krążka Airy\u0026rsquo;ego jest determinowany przez długość fali światła i efektywną średnicę otworu (liczbę f). Żaden projekt optyczny nie jest w stanie \u0026ldquo;obejść\u0026rdquo; tego zjawiska. Dlatego często mówi się o \u0026ldquo;granicy dyfrakcyjnej\u0026rdquo; jako teoretycznym maksimum rozdzielczości, jakie może osiągnąć dany układ optyczny przy danym otworze. Fotografowie muszą więc być świadomi, że dążenie do maksymalnej głębi ostrości poprzez ekstremalne przymykanie przysłony zawsze wiąże się z kompromisem w postaci utraty ogólnej ostrości obrazu.\nPercepcja wpływu dyfrakcji zależy również od tego, jak rozmiar krążka Airy\u0026rsquo;ego ma się do rozmiaru pojedynczego piksela na matrycy. Jeśli krążek Airy\u0026rsquo;ego jest znacznie mniejszy niż rozmiar jednego piksela, to dyfrakcja nie jest głównym czynnikiem ograniczającym rozdzielczość systemu – ograniczeniem jest wówczas rozmiar piksela (zdolność matrycy do próbkowania obrazu). Jeśli jednak krążek Airy\u0026rsquo;ego staje się porównywalny lub większy od rozmiaru piksela (lub obejmuje kilka pikseli), to obraz jest \u0026ldquo;rozmyty przez dyfrakcję\u0026rdquo;, a system osiągnął swoją granicę dyfrakcyjną. Dalsze zmniejszanie rozmiaru pikseli (zwiększanie ich gęstości) nie przyniesie już wzrostu rzeczywistej rozdzielczości przy tej wartości przysłony, ponieważ informacja o drobniejszych detalach jest tracona z powodu dyfrakcyjnego rozmycia, zanim dotrze do pikseli. Dla każdej matrycy (o określonym rozmiarze piksela) istnieje pewna wartość przysłony (tzw. \u0026ldquo;diffraction limited aperture\u0026rdquo; - DLA), powyżej której (czyli przy mniejszych otworach/większych liczbach f) dyfrakcja zaczyna degradować obraz w stopniu większym niż wynikałoby to z samego rozmiaru piksela.\n"},{"id":10,"href":"/photography-basics/docs/blysk/","title":"Jak działa światło błyskowe","section":"Docs","content":" Jak działa światło błyskowe # Lampa błyskowa jest nieocenionym narzędziem w fotografii, pozwalającym na oświetlenie sceny w warunkach niedostatecznego światła zastanego, zamrożenie ruchu, a także kreatywne modelowanie światła i cienia. Zrozumienie zasady jej działania oraz kluczowych parametrów pozwala na pełne wykorzystanie jej możliwości.\nPodstawy działania lampy błyskowej # Sercem większości fotograficznych lamp błyskowych, od małych wbudowanych w aparaty kompaktowe po potężne studyjne jednostki, jest palnik ksenonowy i układ zasilający oparty na kondensatorze.\nBudowa palnika ksenonowego: Palnik ksenonowy to zazwyczaj niewielka, prosta lub ukształtowana (np. w literę U) rurka wykonana ze szkła kwarcowego, wypełniona gazem szlachetnym – ksenonem – pod odpowiednio dobranym ciśnieniem. Na obu końcach rurki znajdują się główne elektrody (anoda i katoda), a często wokół rurki lub w jej pobliżu umieszczona jest dodatkowa elektroda wyzwalająca. Rola kondensatora: Kluczowym elementem układu zasilającego lampę błyskową jest kondensator o dużej pojemności. Jest on ładowany do stosunkowo wysokiego napięcia (kilkaset woltów) z baterii aparatu lub dedykowanego źródła zasilania lampy. Kondensator pełni rolę magazynu energii elektrycznej, która następnie zostanie gwałtownie rozładowana przez palnik w celu wygenerowania błysku. Proces jonizacji gazu i emisji światła: Działanie lampy błyskowej przebiega w kilku etapach: Ładowanie kondensatora głównego: Po włączeniu lampy lub po poprzednim błysku, układ elektroniczny ładuje kondensator główny do wymaganego napięcia. Czas ładowania zależy od pojemności kondensatora, mocy źródła zasilania i stopnia rozładowania po poprzednim błysku. Impuls wyzwalający (trigger): Gdy fotograf naciska spust migawki (lub gdy aparat wysyła sygnał synchronizacji), specjalny układ wyzwalający (zazwyczaj mały transformator impulsowy, tzw. cewka wyzwalająca) generuje bardzo krótki impuls o bardzo wysokim napięciu (rzędu kilku do kilkunastu kilowoltów). Ten impuls jest podawany na elektrodę wyzwalającą palnika. Jonizacja ksenonu: Wysokonapięciowy impuls wyzwalający powoduje jonizację niewielkiej ilości gazu ksenonowego wewnątrz rurki palnika, tworząc w ten sposób przewodzącą ścieżkę (kanał plazmowy) między głównymi elektrodami. Zjonizowany gaz ma znacznie niższą rezystancję niż gaz w stanie neutralnym. Wyładowanie główne: Utworzenie przewodzącego kanału w ksenonie pozwala na gwałtowne rozładowanie energii zgromadzonej w głównym kondensatorze przez palnik. Przez zjonizowany gaz przepływa prąd o dużym natężeniu. Ten przepływ prądu powoduje silne wzbudzenie atomów ksenonu, które następnie, wracając do stanu podstawowego, emitują bardzo jasny, krótkotrwały impuls światła. Czas trwania tego błysku jest bardzo krótki, typowo od około 1/1000 sekundy (dla pełnej mocy) do nawet 1/50000 sekundy lub krócej (dla niższych poziomów mocy w zaawansowanych lampach). Światło emitowane przez palnik ksenonowy ma szerokie spektrum, zbliżone do charakterystyki światła dziennego, co ułatwia uzyskanie naturalnych kolorów na zdjęciach. Kluczowe parametry i pojęcia # Aby efektywnie korzystać z lampy błyskowej, należy zrozumieć kilka kluczowych parametrów i pojęć.\nLiczba przewodnia (GN – Guide Number) # Liczba przewodnia jest podstawowym parametrem określającym moc (a właściwie maksymalny zasięg efektywny) lampy błyskowej.\nDefinicja: Jest to wartość liczbowa, standardowo podawana dla czułości ISO 100 i określonego kąta rozsyłu światła przez palnik lampy (często odpowiadającego pokryciu pola widzenia dla obiektywu o ogniskowej 50 mm lub 105 mm na pełnej klatce). Liczba przewodnia (GN) jest zdefiniowana wzorem: GN=Odległosˊcˊ×Liczba_f gdzie Odległość to dystans od lampy do oświetlanego obiektu (wyrażony w metrach lub stopach, w zależności od specyfikacji lampy), a Liczba_f to wartość przysłony zapewniająca prawidłową ekspozycję przy ISO 100 i pełnej mocy błysku. Jak wykorzystać GN: Znajomość liczby przewodniej pozwala na manualne obliczenie prawidłowych ustawień ekspozycji: Aby obliczyć wymaganą wartość przysłony dla danej odległości od obiektu: Liczba_f=GN/Odległosˊcˊ 79 Aby obliczyć maksymalny efektywny zasięg błysku przy danej wartości przysłony: Odległosˊcˊ=GN/Liczba_f 79 Wpływ ISO na efektywną liczbę przewodnią: Zwiększenie czułości ISO matrycy pozwala na użycie mniejszej mocy błysku (jeśli lampa ma regulację mocy) lub na oświetlenie obiektu znajdującego się w większej odległości. Efektywna liczba przewodnia (GN_eff) rośnie wraz ze wzrostem czułości ISO. Zależność tę można przybliżyć wzorem: GNeff​=GNISO100​⋅(ISOaktualne​/ISO100​)​ Na przykład, podwojenie czułości ISO (np. z ISO 100 na ISO 200) zwiększa efektywną liczbę przewodnią o czynnik 2​≈1.4. Oznacza to, że przy ISO 200 możemy użyć przysłony o jeden stopień mniejszej (np. f/5.6 zamiast f/4) lub oświetlić obiekt znajdujący się 1.4 raza dalej niż przy ISO 100, przy zachowaniu tej samej ekspozycji błysku. Czas synchronizacji (X-sync) # Czas synchronizacji, często oznaczany jako X-sync, jest kluczowym parametrem związanym z użyciem lampy błyskowej w aparatach z migawką szczelinową (najczęściej spotykaną w lustrzankach cyfrowych i wielu aparatach bezlusterkowych).\nDefinicja: Jest to najkrótszy czas otwarcia migawki, przy którym cała powierzchnia matrycy światłoczułej jest w pełni odsłonięta dokładnie w momencie, gdy lampa błyskowa emituje swój główny, krótkotrwały impuls światła. Migawka szczelinowa składa się z dwóch kurtyn (pierwszej i drugiej), które przesuwają się przed matrycą. Przy krótkich czasach naświetlania, druga kurtyna zaczyna zamykać szczelinę, zanim pierwsza kurtyna w pełni ją otworzy – szczelina o stałej szerokości przesuwa się wówczas wzdłuż matrycy. Czas X-sync to najkrótszy czas, przy którym szczelina jest na tyle szeroka, że na moment odsłania całą matrycę. Typowe wartości X-sync to 1/125 s, 1/200 s, 1/250 s, w zależności od modelu aparatu. Konsekwencje użycia czasu naświetlania krótszego niż X-sync (bez trybu HSS): Jeśli ustawimy w aparacie czas naświetlania krótszy niż jego natywny czas X-sync (a lampa i aparat nie obsługują trybu HSS, o którym mowa niżej), to w momencie błysku tylko część matrycy będzie odsłonięta. Skutkiem tego będzie zdjęcie, na którym część kadru (zazwyczaj poziomy pas) będzie prawidłowo naświetlona błyskiem, a reszta pozostanie ciemna (naświetlona tylko światłem zastanym) lub całkowicie czarna. Synchronizacja na pierwszą i drugą kurtynę migawki # W przypadku używania czasów naświetlania dłuższych niż czas trwania błysku (co jest typową sytuacją), fotograf ma możliwość wyboru, w którym momencie długiej ekspozycji ma nastąpić błysk lampy.\nSynchronizacja na pierwszą kurtynę (Front-curtain sync): Jest to standardowy tryb synchronizacji w większości aparatów. Lampa błyskowa wyzwalana jest natychmiast po pełnym otwarciu pierwszej kurtyny migawki (czyli na początku czasu naświetlania). Jeśli fotografujemy poruszający się obiekt przy długim czasie naświetlania, efekt będzie taki, że obiekt zostanie \u0026ldquo;zamrożony\u0026rdquo; przez błysk na początku swojej drogi, a następnie zarejestrowane zostaną smugi jego ruchu powstałe w świetle zastanym. Te smugi będą znajdować się przed zamrożonym obrazem obiektu, co często wygląda nienaturalnie. Synchronizacja na drugą kurtynę (Rear-curtain sync): W tym trybie lampa błyskowa wyzwalana jest tuż przed zamknięciem drugiej kurtyny migawki (czyli pod koniec czasu naświetlania) "}]